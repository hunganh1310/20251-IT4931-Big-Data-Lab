{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Silver Layer - Cleaned & Deduplicated Data\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Read Bronze data nh∆∞ streaming source\n",
        "- Clean v√† normalize stock trade data\n",
        "- Deduplicate v·ªõi watermarking\n",
        "- Validate data quality\n",
        "- Write cleaned data to Silver layer\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Silver Layer**: Cleaned, validated, deduplicated data\n",
        "2. **Data Cleaning**: Normalize, validate, enrich\n",
        "3. **Deduplication**: Remove duplicates v·ªõi watermark\n",
        "4. **Watermarking**: Handle late-arriving data\n",
        "5. **Upsert**: Merge new data v·ªõi existing (Iceberg MERGE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingLakehouseSilver\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "BRONZE_TABLE_PATH = \"/warehouse/bronze/trades\"\n",
        "SILVER_TABLE_PATH = \"/warehouse/silver/trades\"\n",
        "\n",
        "print(\"üöÄ Spark Session initialized for Silver Layer!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Read Bronze nh∆∞ Streaming Source\n",
        "\n",
        "### Key Point:\n",
        "Silver layer reads t·ª´ Bronze nh∆∞ **streaming source**, kh√¥ng ph·∫£i batch.\n",
        "This allows real-time processing t·ª´ Bronze ‚Üí Silver.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Bronze nh∆∞ Streaming Source\n",
        "print(\"üì• Exercise 1: Read Bronze nh∆∞ Streaming Source\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ Reading Bronze layer as stream: {BRONZE_TABLE_PATH}\")\n",
        "\n",
        "bronze_stream = spark.readStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .schema(spark.read.parquet(BRONZE_TABLE_PATH).schema) \\\n",
        "    .load(BRONZE_TABLE_PATH)\n",
        "\n",
        "print(\"‚úÖ Bronze stream created!\")\n",
        "print(\"\\nüìã Bronze stream schema:\")\n",
        "bronze_stream.printSchema()\n",
        "\n",
        "print(\"\\nüí° Key insight:\")\n",
        "print(\"   Bronze ‚Üí Silver: Streaming transformation\")\n",
        "print(\"   - Real-time processing\")\n",
        "print(\"   - Continuous data flow\")\n",
        "print(\"   - Low latency\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Data Cleaning & Normalization\n",
        "\n",
        "### Cleaning Steps:\n",
        "1. **Normalize timestamps**: Convert to proper timestamp type\n",
        "2. **Validate prices**: Ensure positive prices\n",
        "3. **Validate volumes**: Ensure positive volumes\n",
        "4. **Normalize symbols**: Uppercase, trim whitespace\n",
        "5. **Add computed fields**: total_value = price * volume\n",
        "6. **Filter invalid data**: Remove records with nulls or invalid values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Cleaning & Normalization\n",
        "print(\"üßπ Exercise 2: Data Cleaning & Normalization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Cleaning and normalizing data:\")\n",
        "\n",
        "cleaned_stream = bronze_stream \\\n",
        "    .filter(\n",
        "        col(\"trade_id\").isNotNull() &\n",
        "        col(\"symbol\").isNotNull() &\n",
        "        col(\"price\").isNotNull() &\n",
        "        col(\"volume\").isNotNull() &\n",
        "        (col(\"price\") > 0) &\n",
        "        (col(\"volume\") > 0)\n",
        "    ) \\\n",
        "    .withColumn(\"symbol\", upper(trim(col(\"symbol\")))) \\\n",
        "    .withColumn(\"trade_type\", upper(trim(col(\"trade_type\")))) \\\n",
        "    .withColumn(\"event_timestamp\", \n",
        "                coalesce(col(\"event_timestamp\"), \n",
        "                        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))) \\\n",
        "    .withColumn(\"total_value\", col(\"price\") * col(\"volume\")) \\\n",
        "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
        "    .select(\n",
        "        col(\"trade_id\"),\n",
        "        col(\"symbol\"),\n",
        "        col(\"price\"),\n",
        "        col(\"volume\"),\n",
        "        col(\"event_timestamp\"),\n",
        "        col(\"trade_type\"),\n",
        "        col(\"exchange\"),\n",
        "        col(\"total_value\"),\n",
        "        col(\"ingestion_timestamp\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Cleaning transformations applied!\")\n",
        "print(\"\\nüìã Cleaned stream schema:\")\n",
        "cleaned_stream.printSchema()\n",
        "\n",
        "print(\"\\nüí° Cleaning operations:\")\n",
        "print(\"   ‚úÖ Filter invalid records (nulls, negative prices/volumes)\")\n",
        "print(\"   ‚úÖ Normalize symbols (uppercase, trim)\")\n",
        "print(\"   ‚úÖ Normalize timestamps\")\n",
        "print(\"   ‚úÖ Add computed fields (total_value)\")\n",
        "print(\"   ‚úÖ Add ingestion timestamp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Deduplication v·ªõi Watermarking\n",
        "\n",
        "### Key Concepts:\n",
        "- **Watermark**: Threshold for late-arriving data\n",
        "- **Deduplication**: Remove duplicate records\n",
        "- **Event-time**: Use event_timestamp (not processing time)\n",
        "- **Late data**: Data arriving after watermark is dropped\n",
        "\n",
        "### Strategy:\n",
        "- Watermark: 5 minutes (data older than 5 min is considered late)\n",
        "- Deduplicate by: trade_id (unique identifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deduplication v·ªõi Watermarking\n",
        "print(\"üîç Exercise 3: Deduplication v·ªõi Watermarking\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Adding watermark (5 minutes):\")\n",
        "\n",
        "# Add watermark for late data handling\n",
        "with_watermark = cleaned_stream \\\n",
        "    .withWatermark(\"event_timestamp\", \"5 minutes\")\n",
        "\n",
        "print(\"‚úÖ Watermark added!\")\n",
        "print(\"   - Late data threshold: 5 minutes\")\n",
        "print(\"   - Data older than 5 min from latest event is dropped\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Deduplicating by trade_id:\")\n",
        "\n",
        "deduplicated = with_watermark \\\n",
        "    .dropDuplicates([\"trade_id\", \"event_timestamp\"])\n",
        "\n",
        "print(\"‚úÖ Deduplication applied!\")\n",
        "print(\"   - Deduplicate by: trade_id + event_timestamp\")\n",
        "print(\"   - Keeps first occurrence of each trade_id\")\n",
        "\n",
        "print(\"\\nüí° Watermarking benefits:\")\n",
        "print(\"   ‚úÖ Handles late-arriving data\")\n",
        "print(\"   ‚úÖ Bounds state size (drops old data)\")\n",
        "print(\"   ‚úÖ Enables Append mode v·ªõi aggregations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Write to Silver Layer\n",
        "\n",
        "### Output Mode: Append\n",
        "- With watermark, Append mode works\n",
        "- Only new records are written\n",
        "- Deduplication ensures no duplicates\n",
        "\n",
        "### Note:\n",
        "In production with Iceberg, use MERGE for upsert:\n",
        "```python\n",
        ".foreachBatch(lambda df, batchId:\n",
        "    df.createOrReplaceTempView(\"updates\")\n",
        "    spark.sql(\"\"\"\n",
        "        MERGE INTO silver_trades AS t\n",
        "        USING updates AS u\n",
        "        ON t.trade_id = u.trade_id\n",
        "        WHEN MATCHED THEN UPDATE SET *\n",
        "        WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Silver Layer\n",
        "print(\"üíæ Exercise 4: Write to Silver Layer\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ Writing to Silver layer: {SILVER_TABLE_PATH}\")\n",
        "\n",
        "silver_query = deduplicated \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", SILVER_TABLE_PATH) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/silver_checkpoint\") \\\n",
        "    .trigger(processingTime='10 seconds') \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Silver streaming query started!\")\n",
        "print(f\"   Writing to: {SILVER_TABLE_PATH}\")\n",
        "print(f\"   Checkpoint: /tmp/silver_checkpoint\")\n",
        "print(f\"   Output mode: Append (with watermark)\")\n",
        "\n",
        "print(\"\\nüí° Silver layer characteristics:\")\n",
        "print(\"   ‚úÖ Cleaned and validated data\")\n",
        "print(\"   ‚úÖ Deduplicated\")\n",
        "print(\"   ‚úÖ Normalized\")\n",
        "print(\"   ‚úÖ Ready for analytics\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Query is running. To stop: silver_query.stop()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Verify Silver Data\n",
        "\n",
        "### Compare Bronze vs Silver:\n",
        "- Data quality improvements\n",
        "- Deduplication results\n",
        "- Schema differences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Silver Data\n",
        "print(\"üîç Exercise 5: Verify Silver Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "time.sleep(15)  # Wait for data\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Reading Silver data:\")\n",
        "\n",
        "try:\n",
        "    silver_df = spark.read.parquet(SILVER_TABLE_PATH)\n",
        "    bronze_df = spark.read.parquet(BRONZE_TABLE_PATH)\n",
        "    \n",
        "    print(f\"‚úÖ Silver data found!\")\n",
        "    print(f\"   Bronze records: {bronze_df.count()}\")\n",
        "    print(f\"   Silver records: {silver_df.count()}\")\n",
        "    \n",
        "    print(\"\\n2Ô∏è‚É£ Sample Silver data:\")\n",
        "    silver_df.show(10, truncate=False)\n",
        "    \n",
        "    print(\"\\n3Ô∏è‚É£ Data quality check:\")\n",
        "    print(f\"   Null trade_ids: {silver_df.filter(col('trade_id').isNull()).count()}\")\n",
        "    print(f\"   Null prices: {silver_df.filter(col('price').isNull()).count()}\")\n",
        "    print(f\"   Negative prices: {silver_df.filter(col('price') <= 0).count()}\")\n",
        "    \n",
        "    print(\"\\n4Ô∏è‚É£ Records by symbol:\")\n",
        "    silver_df.groupBy(\"symbol\").count().orderBy(desc(\"count\")).show()\n",
        "    \n",
        "    print(\"\\n5Ô∏è‚É£ Price statistics:\")\n",
        "    silver_df.select(\"symbol\", \"price\", \"volume\", \"total_value\").summary().show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error: {e}\")\n",
        "    print(\"   Make sure Silver query has processed some batches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. **Silver Layer**: Cleaned, validated, deduplicated data\n",
        "2. **Streaming from Bronze**: Read Bronze nh∆∞ streaming source\n",
        "3. **Data Cleaning**: Normalize, validate, enrich data\n",
        "4. **Deduplication**: Remove duplicates v·ªõi watermark\n",
        "5. **Watermarking**: Handle late-arriving data\n",
        "6. **Append Mode**: With watermark, Append mode works\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "- **Silver = Clean**: Validated, normalized, deduplicated\n",
        "- **Streaming Transform**: Bronze ‚Üí Silver l√† streaming transformation\n",
        "- **Watermarking**: Essential cho late data v√† state management\n",
        "- **Ready for Analytics**: Silver data ready for Gold layer\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Lab 4: Gold Layer (aggregations, features)\n",
        "- Lab 5: Unified batch + streaming\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
