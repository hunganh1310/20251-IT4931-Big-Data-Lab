{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Bronze Layer - Raw Streaming Data\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Ingest stock trade data t·ª´ Kafka v√†o Iceberg Bronze table\n",
        "- Setup Spark Structured Streaming v·ªõi Kafka source\n",
        "- Write streaming data to Iceberg tables\n",
        "- Understand checkpointing v√† fault tolerance\n",
        "- Verify raw data trong Bronze layer\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Bronze Layer**: Raw, unprocessed data t·ª´ source\n",
        "2. **Kafka Source**: Real-time event stream\n",
        "3. **Iceberg Sink**: Write streaming data to Iceberg\n",
        "4. **Checkpointing**: Fault tolerance cho streaming\n",
        "5. **Schema**: Stock trade event structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow kafka-python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "import json\n",
        "import time\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session for Streaming Lakehouse\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingLakehouseBronze\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/bronze_checkpoint\") \\\n",
        "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"üöÄ Spark Streaming Session initialized!\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Master URL: {spark.sparkContext.master}\")\n",
        "\n",
        "# Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
        "KAFKA_TOPIC = \"stock-trades\"\n",
        "BRONZE_TABLE_PATH = \"/warehouse/bronze/trades\"\n",
        "\n",
        "print(f\"\\nüì° Configuration:\")\n",
        "print(f\"   Kafka: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"   Topic: {KAFKA_TOPIC}\")\n",
        "print(f\"   Bronze Path: {BRONZE_TABLE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Define Stock Trade Schema\n",
        "\n",
        "### Stock Trade Event Schema\n",
        "```json\n",
        "{\n",
        "  \"trade_id\": \"TRD_001\",\n",
        "  \"symbol\": \"AAPL\",\n",
        "  \"price\": 175.50,\n",
        "  \"volume\": 100,\n",
        "  \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "  \"trade_type\": \"BUY\",\n",
        "  \"exchange\": \"NASDAQ\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Stock Trade Schema\n",
        "print(\"üìã Exercise 1: Define Stock Trade Schema\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trade_schema = StructType([\n",
        "    StructField(\"trade_id\", StringType()),\n",
        "    StructField(\"symbol\", StringType()),\n",
        "    StructField(\"price\", DoubleType()),\n",
        "    StructField(\"volume\", IntegerType()),\n",
        "    StructField(\"timestamp\", StringType()),\n",
        "    StructField(\"trade_type\", StringType()),\n",
        "    StructField(\"exchange\", StringType())\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Stock Trade Schema defined:\")\n",
        "trade_schema.printTreeString()\n",
        "\n",
        "print(\"\\nüí° Note: Bronze layer stores raw data as-is\")\n",
        "print(\"   - No transformations\")\n",
        "print(\"   - No validations\")\n",
        "print(\"   - Preserves original format for replay capability\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Read from Kafka Stream\n",
        "\n",
        "### Steps:\n",
        "1. Read stream t·ª´ Kafka topic\n",
        "2. Parse JSON messages\n",
        "3. Extract v√† cast fields\n",
        "4. Add Kafka metadata (partition, offset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from Kafka Stream\n",
        "print(\"üì• Exercise 2: Read from Kafka Stream\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ Reading from Kafka topic: {KAFKA_TOPIC}\")\n",
        "\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .option(\"failOnDataLoss\", \"false\") \\\n",
        "    .load()\n",
        "\n",
        "print(\"‚úÖ Kafka stream created!\")\n",
        "print(\"\\nüìã Kafka stream schema:\")\n",
        "kafka_stream.printSchema()\n",
        "\n",
        "# Parse JSON from Kafka value\n",
        "print(\"\\n2Ô∏è‚É£ Parsing JSON messages:\")\n",
        "\n",
        "parsed_stream = kafka_stream \\\n",
        "    .select(\n",
        "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
        "        col(\"value\").cast(\"string\").alias(\"json_value\"),\n",
        "        col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
        "        col(\"partition\"),\n",
        "        col(\"offset\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"kafka_key\"),\n",
        "        from_json(col(\"json_value\"), trade_schema).alias(\"data\"),\n",
        "        col(\"kafka_timestamp\"),\n",
        "        col(\"partition\"),\n",
        "        col(\"offset\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"kafka_key\"),\n",
        "        col(\"data.*\"),\n",
        "        col(\"kafka_timestamp\").alias(\"kafka_ingestion_time\"),\n",
        "        col(\"partition\").alias(\"kafka_partition\"),\n",
        "        col(\"offset\").alias(\"kafka_offset\")\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"event_timestamp\",\n",
        "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Parsed stream schema:\")\n",
        "parsed_stream.printSchema()\n",
        "\n",
        "print(\"\\nüí° Bronze layer includes:\")\n",
        "print(\"   - Original trade data (from Kafka)\")\n",
        "print(\"   - Kafka metadata (partition, offset, ingestion time)\")\n",
        "print(\"   - Event timestamp (from data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Write to Iceberg Bronze Table\n",
        "\n",
        "### Key Points:\n",
        "- **Format**: Iceberg (unified storage)\n",
        "- **Checkpointing**: For fault tolerance\n",
        "- **Trigger**: Processing time (e.g., 10 seconds)\n",
        "- **Output Mode**: Append (new data only)\n",
        "\n",
        "### Note:\n",
        "Full Iceberg functionality requires Iceberg Spark runtime JAR.\n",
        "For demo, we'll show the pattern. In production, configure:\n",
        "```python\n",
        ".config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
        ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Iceberg Bronze Table\n",
        "print(\"üíæ Exercise 3: Write to Iceberg Bronze Table\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ Writing to Bronze layer: {BRONZE_TABLE_PATH}\")\n",
        "\n",
        "# For demo: Write to Parquet (Iceberg pattern)\n",
        "# In production with Iceberg JAR, use:\n",
        "# .format(\"iceberg\")\n",
        "# .option(\"path\", BRONZE_TABLE_PATH)\n",
        "# .option(\"checkpointLocation\", \"/tmp/bronze_checkpoint\")\n",
        "\n",
        "bronze_query = parsed_stream \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", BRONZE_TABLE_PATH) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/bronze_checkpoint\") \\\n",
        "    .trigger(processingTime='10 seconds') \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Bronze streaming query started!\")\n",
        "print(f\"   Writing to: {BRONZE_TABLE_PATH}\")\n",
        "print(f\"   Checkpoint: /tmp/bronze_checkpoint\")\n",
        "print(f\"   Trigger: Every 10 seconds\")\n",
        "\n",
        "print(\"\\nüí° With Iceberg JAR, use:\")\n",
        "print(\"   .format('iceberg')\")\n",
        "print(\"   .option('path', 'warehouse.bronze.trades')\")\n",
        "print(\"   .option('checkpointLocation', '/tmp/bronze_checkpoint')\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Query is running. To stop: bronze_query.stop()\")\n",
        "print(\"   To check status: bronze_query.status\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Verify Bronze Data\n",
        "\n",
        "### Check:\n",
        "- Data is being written\n",
        "- Schema is correct\n",
        "- Kafka metadata is preserved\n",
        "- Event timestamps are correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Bronze Data\n",
        "print(\"üîç Exercise 4: Verify Bronze Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Wait a bit for data to arrive\n",
        "print(\"\\n‚è≥ Waiting for data to be written...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# Read Bronze data (batch read)\n",
        "print(\"\\n1Ô∏è‚É£ Reading Bronze data:\")\n",
        "\n",
        "try:\n",
        "    bronze_df = spark.read.parquet(BRONZE_TABLE_PATH)\n",
        "    \n",
        "    print(f\"‚úÖ Bronze data found!\")\n",
        "    print(f\"   Total records: {bronze_df.count()}\")\n",
        "    \n",
        "    print(\"\\n2Ô∏è‚É£ Sample Bronze data:\")\n",
        "    bronze_df.show(10, truncate=False)\n",
        "    \n",
        "    print(\"\\n3Ô∏è‚É£ Schema:\")\n",
        "    bronze_df.printSchema()\n",
        "    \n",
        "    print(\"\\n4Ô∏è‚É£ Data summary:\")\n",
        "    bronze_df.select(\"symbol\", \"price\", \"volume\", \"trade_type\").summary().show()\n",
        "    \n",
        "    print(\"\\n5Ô∏è‚É£ Records by symbol:\")\n",
        "    bronze_df.groupBy(\"symbol\").count().orderBy(desc(\"count\")).show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error reading Bronze data: {e}\")\n",
        "    print(\"   Make sure:\")\n",
        "    print(\"   1. Kafka producer is running (stock_trade_simulator.py)\")\n",
        "    print(\"   2. Streaming query has processed some batches\")\n",
        "    print(\"   3. Wait a bit longer for data to arrive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. **Bronze Layer**: Raw, unprocessed data t·ª´ Kafka\n",
        "2. **Kafka Integration**: Read streaming data t·ª´ Kafka topics\n",
        "3. **Schema Definition**: Define structure cho stock trade events\n",
        "4. **Iceberg Write**: Write streaming data to Iceberg (pattern)\n",
        "5. **Checkpointing**: Fault tolerance cho streaming queries\n",
        "6. **Verification**: Check data trong Bronze layer\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "- **Bronze = Raw**: No transformations, preserves original data\n",
        "- **Replay Capability**: C√≥ th·ªÉ reprocess t·ª´ Bronze n·∫øu c·∫ßn\n",
        "- **Kafka Metadata**: Preserve partition, offset, ingestion time\n",
        "- **Unified Storage**: Iceberg cho ph√©p query real-time v√† historical\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Lab 3: Silver Layer (cleaning, deduplication)\n",
        "- Lab 4: Gold Layer (aggregations)\n",
        "- Lab 5: Unified batch + streaming\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
