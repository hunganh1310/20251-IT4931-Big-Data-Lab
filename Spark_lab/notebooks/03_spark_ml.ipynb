{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Spark MLlib - Machine Learning\n",
        "\n",
        "## ğŸ¯ **Learning Objectives:**\n",
        "- Master Spark MLlib machine learning library\n",
        "- Learn ML pipeline concepts and workflows\n",
        "- Practice feature engineering techniques\n",
        "- Understand model training and evaluation\n",
        "- Implement real-world ML use cases\n",
        "\n",
        "## ğŸ“š **Key Concepts:**\n",
        "1. **MLlib**: Spark's machine learning library\n",
        "2. **ML Pipelines**: End-to-end ML workflows\n",
        "3. **Feature Engineering**: Data transformation for ML\n",
        "4. **Model Training**: Supervised and unsupervised learning\n",
        "5. **Model Evaluation**: Performance metrics and validation\n",
        "\n",
        "## ğŸ—ï¸ **Architecture Overview:**\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Raw Data      â”‚â”€â”€â”€â–¶â”‚ Feature Pipeline â”‚â”€â”€â”€â–¶â”‚   ML Models     â”‚\n",
        "â”‚   (CSV/JSON)    â”‚    â”‚ â€¢ Transformers   â”‚    â”‚ â€¢ Classificationâ”‚\n",
        "â”‚                 â”‚    â”‚ â€¢ Estimators    â”‚    â”‚ â€¢ Regression    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Clustering   â”‚\n",
        "         â”‚                        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â–¼                        â–¼                        â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â–¼\n",
        "â”‚ Data Preprocessingâ”‚    â”‚ Feature Selectionâ”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ â€¢ Cleaning       â”‚    â”‚ â€¢ Scaling        â”‚    â”‚ Model Evaluationâ”‚\n",
        "â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Encoding       â”‚    â”‚ â€¢ Metrics       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Validation   â”‚\n",
        "                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## ğŸ¤– **ML Use Cases:**\n",
        "- **Classification**: Customer segmentation, fraud detection\n",
        "- **Regression**: Price prediction, demand forecasting\n",
        "- **Clustering**: Market segmentation, anomaly detection\n",
        "- **Recommendation**: Product recommendations, content filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow scikit-learn matplotlib seaborn\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.regression import *\n",
        "from pyspark.ml.clustering import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.tuning import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "print(\"âœ… Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session for MLlib\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkMLlibLab\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"ğŸš€ Spark MLlib Session initialized successfully!\")\n",
        "print(f\"ğŸ“Š Spark Version: {spark.version}\")\n",
        "print(f\"ğŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ğŸ¤– MLlib Version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Sample ML Datasets\n",
        "print(\"ğŸ“Š Creating sample datasets for ML experiments...\")\n",
        "\n",
        "# 1. Customer Classification Dataset\n",
        "print(\"\\nğŸ‘¥ Creating customer classification dataset...\")\n",
        "\n",
        "customer_data = []\n",
        "customer_types = ['Premium', 'Standard', 'Basic']\n",
        "regions = ['North', 'South', 'East', 'West']\n",
        "products = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
        "\n",
        "for i in range(1000):\n",
        "    age = random.randint(18, 80)\n",
        "    income = random.randint(20000, 150000)\n",
        "    spending = random.randint(100, 5000)\n",
        "    \n",
        "    # Create features that influence customer type\n",
        "    if income > 80000 and spending > 2000:\n",
        "        customer_type = 'Premium'\n",
        "    elif income > 40000 and spending > 800:\n",
        "        customer_type = 'Standard'\n",
        "    else:\n",
        "        customer_type = 'Basic'\n",
        "    \n",
        "    customer_data.append({\n",
        "        'customer_id': f'CUST_{i+1:04d}',\n",
        "        'age': age,\n",
        "        'income': income,\n",
        "        'spending': spending,\n",
        "        'region': random.choice(regions),\n",
        "        'product_category': random.choice(products),\n",
        "        'customer_type': customer_type,\n",
        "        'satisfaction_score': random.randint(1, 10),\n",
        "        'years_customer': random.randint(1, 20)\n",
        "    })\n",
        "\n",
        "# 2. Sales Regression Dataset\n",
        "print(\"\\nğŸ’° Creating sales regression dataset...\")\n",
        "\n",
        "sales_data = []\n",
        "for i in range(800):\n",
        "    # Features that influence sales\n",
        "    marketing_budget = random.randint(1000, 50000)\n",
        "    price = random.uniform(10, 500)\n",
        "    season = random.choice(['Spring', 'Summer', 'Fall', 'Winter'])\n",
        "    promotion = random.choice(['None', 'Discount', 'BOGO', 'Free_Shipping'])\n",
        "    \n",
        "    # Calculate sales based on features\n",
        "    base_sales = marketing_budget * 0.1 + random.randint(100, 1000)\n",
        "    \n",
        "    if season == 'Summer':\n",
        "        base_sales *= 1.2\n",
        "    elif season == 'Winter':\n",
        "        base_sales *= 1.1\n",
        "    \n",
        "    if promotion == 'Discount':\n",
        "        base_sales *= 1.3\n",
        "    elif promotion == 'BOGO':\n",
        "        base_sales *= 1.5\n",
        "    elif promotion == 'Free_Shipping':\n",
        "        base_sales *= 1.1\n",
        "    \n",
        "    # Add some noise\n",
        "    sales = base_sales + random.randint(-200, 200)\n",
        "    sales = max(0, sales)  # Ensure non-negative\n",
        "    \n",
        "    sales_data.append({\n",
        "        'product_id': f'PROD_{i+1:04d}',\n",
        "        'marketing_budget': marketing_budget,\n",
        "        'price': round(price, 2),\n",
        "        'season': season,\n",
        "        'promotion': promotion,\n",
        "        'sales': round(sales, 2),\n",
        "        'competitor_price': round(price * random.uniform(0.8, 1.2), 2),\n",
        "        'inventory_level': random.randint(10, 1000)\n",
        "    })\n",
        "\n",
        "# 3. Customer Clustering Dataset\n",
        "print(\"\\nğŸ” Creating customer clustering dataset...\")\n",
        "\n",
        "clustering_data = []\n",
        "for i in range(600):\n",
        "    # Generate features for clustering\n",
        "    age = random.randint(18, 70)\n",
        "    income = random.randint(20000, 120000)\n",
        "    spending_frequency = random.randint(1, 30)  # times per month\n",
        "    avg_order_value = random.randint(20, 500)\n",
        "    \n",
        "    clustering_data.append({\n",
        "        'customer_id': f'CLUST_{i+1:04d}',\n",
        "        'age': age,\n",
        "        'income': income,\n",
        "        'spending_frequency': spending_frequency,\n",
        "        'avg_order_value': avg_order_value,\n",
        "        'online_visits': random.randint(1, 100),\n",
        "        'mobile_app_usage': random.randint(0, 50),\n",
        "        'customer_service_calls': random.randint(0, 10)\n",
        "    })\n",
        "\n",
        "print(f\"\\nâœ… Sample datasets created:\")\n",
        "print(f\"   ğŸ‘¥ Customer classification: {len(customer_data)} records\")\n",
        "print(f\"   ğŸ’° Sales regression: {len(sales_data)} records\")\n",
        "print(f\"   ğŸ” Customer clustering: {len(clustering_data)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Spark DataFrames for ML\n",
        "print(\"ğŸ”„ Creating Spark DataFrames for ML experiments...\")\n",
        "\n",
        "# Convert to Spark DataFrames\n",
        "customers_df = spark.createDataFrame(customer_data)\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "clustering_df = spark.createDataFrame(clustering_data)\n",
        "\n",
        "print(\"\\nğŸ“Š Customer Classification DataFrame:\")\n",
        "customers_df.printSchema()\n",
        "customers_df.show(5)\n",
        "\n",
        "print(\"\\nğŸ’° Sales Regression DataFrame:\")\n",
        "sales_df.printSchema()\n",
        "sales_df.show(5)\n",
        "\n",
        "print(\"\\nğŸ” Customer Clustering DataFrame:\")\n",
        "clustering_df.printSchema()\n",
        "clustering_df.show(5)\n",
        "\n",
        "print(f\"\\nâœ… ML DataFrames created successfully!\")\n",
        "print(f\"   ğŸ‘¥ Customers: {customers_df.count()} records\")\n",
        "print(f\"   ğŸ’° Sales: {sales_df.count()} records\")\n",
        "print(f\"   ğŸ” Clustering: {clustering_df.count()} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Customer Classification\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Master classification algorithms in MLlib\n",
        "- Learn feature engineering for ML\n",
        "- Practice ML pipeline creation\n",
        "- Understand model evaluation metrics\n",
        "\n",
        "### ğŸ“š **Key Concepts:**\n",
        "1. **Classification**: Predicting categorical outcomes\n",
        "2. **Feature Engineering**: Preparing data for ML\n",
        "3. **ML Pipelines**: End-to-end ML workflows\n",
        "4. **Model Evaluation**: Performance metrics and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Customer Classification\n",
        "print(\"ğŸ¯ Exercise 1: Customer Classification\")\n",
        "\n",
        "print(\"\\nğŸ”§ Setting up feature engineering pipeline...\")\n",
        "\n",
        "# Define feature columns\n",
        "feature_columns = ['age', 'income', 'spending', 'satisfaction_score', 'years_customer']\n",
        "categorical_columns = ['region', 'product_category']\n",
        "\n",
        "# Create feature vector assembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_columns,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Create string indexers for categorical variables\n",
        "region_indexer = StringIndexer(\n",
        "    inputCol=\"region\",\n",
        "    outputCol=\"region_index\"\n",
        ")\n",
        "\n",
        "product_indexer = StringIndexer(\n",
        "    inputCol=\"product_category\", \n",
        "    outputCol=\"product_index\"\n",
        ")\n",
        "\n",
        "# Create label indexer for target variable\n",
        "label_indexer = StringIndexer(\n",
        "    inputCol=\"customer_type\",\n",
        "    outputCol=\"label\"\n",
        ")\n",
        "\n",
        "# Create Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“Š Building ML pipeline...\")\n",
        "\n",
        "# Create ML pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    region_indexer,\n",
        "    product_indexer,\n",
        "    label_indexer,\n",
        "    assembler,\n",
        "    rf_classifier\n",
        "])\n",
        "\n",
        "print(\"\\nğŸ”„ Splitting data into train/test sets...\")\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_data, test_data = customers_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"   ğŸ“Š Training set: {train_data.count()} records\")\n",
        "print(f\"   ğŸ“Š Test set: {test_data.count()} records\")\n",
        "\n",
        "print(\"\\nğŸš€ Training the model...\")\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "print(\"âœ… Model training completed!\")\n",
        "\n",
        "print(\"\\nğŸ“Š Making predictions on test set...\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "print(\"\\nğŸ“ˆ Model evaluation results:\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"   ğŸ¯ Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"   ğŸ“Š F1 Score: {f1_score:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Sample predictions:\")\n",
        "predictions.select(\"customer_id\", \"customer_type\", \"prediction\", \"probability\").show(10)\n",
        "\n",
        "print(\"\\nâœ… Customer classification completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
