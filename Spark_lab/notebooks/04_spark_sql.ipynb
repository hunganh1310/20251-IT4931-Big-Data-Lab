{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Spark SQL\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Master Spark SQL for querying data\n",
        "- Learn to use SQL syntax with Spark\n",
        "- Understand CREATE TABLE, VIEW, and UDF\n",
        "- Practice advanced SQL operations\n",
        "- Learn catalog management\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Spark SQL**: SQL interface for Spark\n",
        "2. **CREATE TABLE**: Managed and external tables\n",
        "3. **VIEW**: Virtual tables from queries\n",
        "4. **UDF**: User Defined Functions\n",
        "5. **Catalog**: Metadata management\n",
        "\n",
        "## üèóÔ∏è **Architecture Overview:**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   DataFrames    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Spark SQL      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   SQL Results   ‚îÇ\n",
        "‚îÇ   (Python API)  ‚îÇ    ‚îÇ   (SQL Interface)‚îÇ    ‚îÇ                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ                        ‚îÇ                        ‚îÇ\n",
        "         ‚ñº                        ‚ñº                        ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Register as     ‚îÇ    ‚îÇ SQL Queries      ‚îÇ    ‚îÇ Tables & Views  ‚îÇ\n",
        "‚îÇ Temporary View  ‚îÇ    ‚îÇ ‚Ä¢ SELECT         ‚îÇ    ‚îÇ ‚Ä¢ Managed       ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ JOIN          ‚îÇ    ‚îÇ ‚Ä¢ External      ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Window Funcs  ‚îÇ    ‚îÇ ‚Ä¢ Views         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## üìä **Use Cases:**\n",
        "- **SQL Queries**: Query DataFrames using SQL syntax\n",
        "- **Table Management**: Create and manage tables\n",
        "- **View Creation**: Create reusable views\n",
        "- **Custom Functions**: Extend SQL with UDFs\n",
        "- **Catalog Operations**: Manage database catalogs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow psycopg2-binary sqlalchemy\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkSQLLab\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"üöÄ Spark SQL Session initialized successfully!\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"üíæ Default Catalog: {spark.catalog.currentCatalog()}\")\n",
        "print(f\"üìÅ Current Database: {spark.catalog.currentDatabase()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Basic Spark SQL Queries\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Query DataFrames using SQL syntax\n",
        "- Register DataFrames as temporary views\n",
        "- Perform basic SQL operations\n",
        "- Understand SQL vs DataFrame API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Sample Data\n",
        "print(\"üìä Creating sample datasets for Spark SQL Lab...\")\n",
        "\n",
        "# Sample Sales Data\n",
        "sales_data = []\n",
        "products = ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Camera', 'Monitor', 'Keyboard', 'Mouse']\n",
        "customers = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry']\n",
        "categories = ['Electronics', 'Accessories', 'Computing']\n",
        "\n",
        "for i in range(1000):\n",
        "    sales_data.append({\n",
        "        'sale_id': f'SALE_{i+1:04d}',\n",
        "        'customer_name': random.choice(customers),\n",
        "        'product_name': random.choice(products),\n",
        "        'category': random.choice(categories),\n",
        "        'quantity': random.randint(1, 5),\n",
        "        'unit_price': round(random.uniform(50, 2000), 2),\n",
        "        'sale_date': (datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d'),\n",
        "        'region': random.choice(['North', 'South', 'East', 'West', 'Central'])\n",
        "    })\n",
        "\n",
        "# Sample Customer Data\n",
        "customer_data = []\n",
        "for customer in customers:\n",
        "    customer_data.append({\n",
        "        'customer_name': customer,\n",
        "        'age': random.randint(25, 65),\n",
        "        'city': random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']),\n",
        "        'membership_level': random.choice(['Bronze', 'Silver', 'Gold', 'Platinum']),\n",
        "        'join_date': (datetime.now() - timedelta(days=random.randint(30, 1000))).strftime('%Y-%m-%d'),\n",
        "        'total_purchases': random.randint(5, 50)\n",
        "    })\n",
        "\n",
        "# Create DataFrames\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "customers_df = spark.createDataFrame(customer_data)\n",
        "\n",
        "print(f\"‚úÖ Sample data created:\")\n",
        "print(f\"   üìä Sales records: {len(sales_data)}\")\n",
        "print(f\"   üë• Customer records: {len(customer_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register DataFrames as Temporary Views\n",
        "print(\"üìù Registering DataFrames as temporary views for SQL queries...\")\n",
        "\n",
        "sales_df.createOrReplaceTempView(\"sales\")\n",
        "customers_df.createOrReplaceTempView(\"customers\")\n",
        "\n",
        "print(\"‚úÖ Views created:\")\n",
        "print(\"   - sales\")\n",
        "print(\"   - customers\")\n",
        "\n",
        "# List all temporary views\n",
        "print(\"\\nüìã Available temporary views:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic SQL Queries\n",
        "print(\"üîç Exercise 1: Basic Spark SQL Queries\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Simple SELECT query:\")\n",
        "result1 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        sale_id,\n",
        "        customer_name,\n",
        "        product_name,\n",
        "        quantity,\n",
        "        unit_price,\n",
        "        (quantity * unit_price) as total_amount\n",
        "    FROM sales\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result1.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Filtering with WHERE clause:\")\n",
        "result2 = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM sales\n",
        "    WHERE category = 'Electronics'\n",
        "        AND quantity > 2\n",
        "    ORDER BY unit_price DESC\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result2.show(truncate=False)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Aggregations with GROUP BY:\")\n",
        "result3 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        product_name,\n",
        "        COUNT(*) as sale_count,\n",
        "        SUM(quantity) as total_quantity,\n",
        "        AVG(unit_price) as avg_price,\n",
        "        SUM(quantity * unit_price) as total_revenue\n",
        "    FROM sales\n",
        "    GROUP BY product_name\n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "result3.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: JOIN Operations with SQL\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Perform JOINs using SQL syntax\n",
        "- Understand different JOIN types\n",
        "- Combine multiple tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JOIN Operations with SQL\n",
        "print(\"üîó Exercise 2: JOIN Operations with SQL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ INNER JOIN:\")\n",
        "result4 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.sale_id,\n",
        "        s.customer_name,\n",
        "        s.product_name,\n",
        "        s.quantity,\n",
        "        s.unit_price,\n",
        "        c.city,\n",
        "        c.membership_level,\n",
        "        c.age\n",
        "    FROM sales s\n",
        "    INNER JOIN customers c\n",
        "        ON s.customer_name = c.customer_name\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result4.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ LEFT JOIN:\")\n",
        "result5 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        c.customer_name,\n",
        "        c.city,\n",
        "        c.membership_level,\n",
        "        COUNT(s.sale_id) as total_sales,\n",
        "        COALESCE(SUM(s.quantity * s.unit_price), 0) as total_spent\n",
        "    FROM customers c\n",
        "    LEFT JOIN sales s\n",
        "        ON c.customer_name = s.customer_name\n",
        "    GROUP BY c.customer_name, c.city, c.membership_level\n",
        "    ORDER BY total_spent DESC\n",
        "\"\"\")\n",
        "result5.show(truncate=False)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Complex JOIN with Aggregations:\")\n",
        "result6 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        s.region,\n",
        "        s.category,\n",
        "        COUNT(DISTINCT s.customer_name) as unique_customers,\n",
        "        COUNT(*) as total_sales,\n",
        "        SUM(s.quantity * s.unit_price) as total_revenue,\n",
        "        AVG(s.quantity * s.unit_price) as avg_transaction_value\n",
        "    FROM sales s\n",
        "    GROUP BY s.region, s.category\n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "result6.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Window Functions with SQL\n",
        "print(\"ü™ü Exercise 3: Window Functions with SQL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ ROW_NUMBER and RANK:\")\n",
        "result7 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        customer_name,\n",
        "        product_name,\n",
        "        quantity * unit_price as total_amount,\n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY customer_name \n",
        "            ORDER BY quantity * unit_price DESC\n",
        "        ) as row_num,\n",
        "        RANK() OVER (\n",
        "            PARTITION BY customer_name \n",
        "            ORDER BY quantity * unit_price DESC\n",
        "        ) as rank,\n",
        "        DENSE_RANK() OVER (\n",
        "            PARTITION BY customer_name \n",
        "            ORDER BY quantity * unit_price DESC\n",
        "        ) as dense_rank\n",
        "    FROM sales\n",
        "    WHERE customer_name = 'Alice'\n",
        "    ORDER BY total_amount DESC\n",
        "\"\"\")\n",
        "result7.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Running Totals:\")\n",
        "result8 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        sale_id,\n",
        "        customer_name,\n",
        "        sale_date,\n",
        "        quantity * unit_price as transaction_amount,\n",
        "        SUM(quantity * unit_price) OVER (\n",
        "            PARTITION BY customer_name \n",
        "            ORDER BY sale_date \n",
        "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "        ) as running_total\n",
        "    FROM sales\n",
        "    WHERE customer_name = 'Alice'\n",
        "    ORDER BY sale_date\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result8.show(truncate=False)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Moving Average:\")\n",
        "result9 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        sale_date,\n",
        "        product_name,\n",
        "        quantity * unit_price as daily_revenue,\n",
        "        AVG(quantity * unit_price) OVER (\n",
        "            PARTITION BY product_name \n",
        "            ORDER BY sale_date \n",
        "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
        "        ) as moving_avg_3days\n",
        "    FROM sales\n",
        "    WHERE product_name = 'Laptop'\n",
        "    ORDER BY sale_date\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result9.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CREATE VIEW\n",
        "print(\"üëÅÔ∏è Exercise 4: CREATE VIEW\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Create a temporary view for customer sales summary:\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMP VIEW customer_sales_summary AS\n",
        "    SELECT \n",
        "        c.customer_name,\n",
        "        c.city,\n",
        "        c.membership_level,\n",
        "        COUNT(s.sale_id) as total_orders,\n",
        "        SUM(s.quantity * s.unit_price) as total_spent,\n",
        "        AVG(s.quantity * s.unit_price) as avg_order_value,\n",
        "        MAX(s.sale_date) as last_purchase_date\n",
        "    FROM customers c\n",
        "    LEFT JOIN sales s\n",
        "        ON c.customer_name = s.customer_name\n",
        "    GROUP BY c.customer_name, c.city, c.membership_level\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ View 'customer_sales_summary' created!\")\n",
        "\n",
        "# Query the view\n",
        "print(\"\\n2Ô∏è‚É£ Query the view:\")\n",
        "result10 = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM customer_sales_summary\n",
        "    ORDER BY total_spent DESC\n",
        "\"\"\")\n",
        "result10.show(truncate=False)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Create another view for product performance:\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMP VIEW product_performance AS\n",
        "    SELECT \n",
        "        product_name,\n",
        "        category,\n",
        "        COUNT(*) as sale_count,\n",
        "        SUM(quantity) as total_quantity_sold,\n",
        "        SUM(quantity * unit_price) as total_revenue,\n",
        "        AVG(unit_price) as avg_price,\n",
        "        MIN(unit_price) as min_price,\n",
        "        MAX(unit_price) as max_price\n",
        "    FROM sales\n",
        "    GROUP BY product_name, category\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ View 'product_performance' created!\")\n",
        "\n",
        "# Query the new view\n",
        "result11 = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM product_performance\n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "result11.show(truncate=False)\n",
        "\n",
        "# List all views\n",
        "print(\"\\nüìã All available views:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CREATE TABLE\n",
        "print(\"üìä Exercise 5: CREATE TABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Create a managed table from query result:\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TABLE sales_summary AS\n",
        "    SELECT \n",
        "        region,\n",
        "        category,\n",
        "        COUNT(*) as total_sales,\n",
        "        SUM(quantity) as total_quantity,\n",
        "        SUM(quantity * unit_price) as total_revenue,\n",
        "        AVG(quantity * unit_price) as avg_transaction_value\n",
        "    FROM sales\n",
        "    GROUP BY region, category\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Managed table 'sales_summary' created!\")\n",
        "\n",
        "# Query the table\n",
        "result12 = spark.sql(\"SELECT * FROM sales_summary ORDER BY total_revenue DESC\")\n",
        "result12.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Create a table with explicit schema:\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TABLE customer_metrics (\n",
        "        customer_name STRING,\n",
        "        city STRING,\n",
        "        membership_level STRING,\n",
        "        total_orders BIGINT,\n",
        "        total_spent DOUBLE,\n",
        "        avg_order_value DOUBLE\n",
        "    ) USING DELTA\n",
        "    AS\n",
        "    SELECT \n",
        "        c.customer_name,\n",
        "        c.city,\n",
        "        c.membership_level,\n",
        "        COUNT(s.sale_id) as total_orders,\n",
        "        COALESCE(SUM(s.quantity * s.unit_price), 0) as total_spent,\n",
        "        COALESCE(AVG(s.quantity * s.unit_price), 0) as avg_order_value\n",
        "    FROM customers c\n",
        "    LEFT JOIN sales s\n",
        "        ON c.customer_name = s.customer_name\n",
        "    GROUP BY c.customer_name, c.city, c.membership_level\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Table 'customer_metrics' created with explicit schema!\")\n",
        "\n",
        "# Query the table\n",
        "result13 = spark.sql(\"SELECT * FROM customer_metrics ORDER BY total_spent DESC\")\n",
        "result13.show(truncate=False)\n",
        "\n",
        "# List all tables\n",
        "print(\"\\nüìã All available tables:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: User Defined Functions (UDF)\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Create User Defined Functions (UDFs)\n",
        "- Register UDFs for use in SQL\n",
        "- Understand UDF performance considerations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User Defined Functions (UDF)\n",
        "print(\"‚öôÔ∏è Exercise 6: User Defined Functions (UDF)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Create a simple UDF:\")\n",
        "def calculate_discount(total_amount):\n",
        "    \"\"\"Calculate discount based on total amount\"\"\"\n",
        "    if total_amount > 1000:\n",
        "        return total_amount * 0.1\n",
        "    elif total_amount > 500:\n",
        "        return total_amount * 0.05\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# Register UDF\n",
        "from pyspark.sql.types import DoubleType\n",
        "calculate_discount_udf = spark.udf.register(\"calculate_discount\", calculate_discount, DoubleType())\n",
        "\n",
        "print(\"‚úÖ UDF 'calculate_discount' registered!\")\n",
        "\n",
        "# Use UDF in SQL\n",
        "result14 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        sale_id,\n",
        "        customer_name,\n",
        "        quantity * unit_price as total_amount,\n",
        "        calculate_discount(quantity * unit_price) as discount,\n",
        "        quantity * unit_price - calculate_discount(quantity * unit_price) as final_amount\n",
        "    FROM sales\n",
        "    WHERE quantity * unit_price > 500\n",
        "    ORDER BY total_amount DESC\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result14.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Create a UDF for string manipulation:\")\n",
        "def format_customer_name(name, membership):\n",
        "    \"\"\"Format customer name with membership level\"\"\"\n",
        "    return f\"{name} ({membership})\"\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "format_customer_udf = spark.udf.register(\"format_customer\", format_customer_name, StringType())\n",
        "\n",
        "print(\"‚úÖ UDF 'format_customer' registered!\")\n",
        "\n",
        "# Use UDF in SQL\n",
        "result15 = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        format_customer(c.customer_name, c.membership_level) as formatted_name,\n",
        "        COUNT(s.sale_id) as total_orders,\n",
        "        SUM(s.quantity * s.unit_price) as total_spent\n",
        "    FROM customers c\n",
        "    LEFT JOIN sales s\n",
        "        ON c.customer_name = s.customer_name\n",
        "    GROUP BY c.customer_name, c.membership_level\n",
        "    ORDER BY total_spent DESC\n",
        "\"\"\")\n",
        "result15.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Catalog Management\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand Spark catalog system\n",
        "- List databases, tables, and functions\n",
        "- Manage database and table metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catalog Management\n",
        "print(\"üóÇÔ∏è Exercise 7: Catalog Management\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ List all databases:\")\n",
        "spark.sql(\"SHOW DATABASES\").show()\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Show current database:\")\n",
        "print(f\"Current database: {spark.catalog.currentDatabase()}\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ List all tables in current database:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Describe a table:\")\n",
        "spark.sql(\"DESCRIBE EXTENDED sales_summary\").show(truncate=False)\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ List all functions:\")\n",
        "print(\"Available functions (first 20):\")\n",
        "spark.sql(\"SHOW FUNCTIONS\").show(20, truncate=False)\n",
        "\n",
        "print(\"\\n6Ô∏è‚É£ List user-defined functions:\")\n",
        "print(\"User-defined functions:\")\n",
        "spark.sql(\"SHOW USER FUNCTIONS\").show()\n",
        "\n",
        "print(\"\\n7Ô∏è‚É£ Get table details using catalog API:\")\n",
        "tables = spark.catalog.listTables()\n",
        "print(f\"\\nTotal tables: {len(tables)}\")\n",
        "for table in tables:\n",
        "    print(f\"  - {table.name} (type: {table.tableType})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. **Spark SQL Basics**: Query DataFrames using SQL syntax\n",
        "2. **JOIN Operations**: INNER JOIN, LEFT JOIN with SQL\n",
        "3. **Window Functions**: ROW_NUMBER, RANK, running totals, moving averages\n",
        "4. **CREATE VIEW**: Create reusable temporary views\n",
        "5. **CREATE TABLE**: Create managed tables from queries\n",
        "6. **User Defined Functions**: Create and register UDFs for SQL\n",
        "7. **Catalog Management**: Manage databases, tables, and functions\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "- Spark SQL provides a familiar SQL interface for Spark\n",
        "- Views simplify complex queries and improve reusability\n",
        "- UDFs extend SQL functionality with custom logic\n",
        "- Catalog API helps manage metadata and discover resources\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Practice with more complex SQL queries\n",
        "- Explore advanced SQL features (CTEs, subqueries)\n",
        "- Learn about Spark SQL performance optimization\n",
        "- Integrate Spark SQL with external data sources\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
