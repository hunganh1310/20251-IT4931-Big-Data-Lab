{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 6: Spark Streaming v·ªõi Kafka Integration\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Integrate Spark Structured Streaming v·ªõi Kafka\n",
        "- Create Kafka producers ƒë·ªÉ send data\n",
        "- Read streaming data t·ª´ Kafka topics\n",
        "- Process real-time data v·ªõi windows v√† watermarking\n",
        "- Write results back to Kafka ho·∫∑c other sinks\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Kafka Producer**: Send data to Kafka topics\n",
        "2. **Spark Kafka Source**: Read from Kafka in Spark\n",
        "3. **Schema Parsing**: Parse JSON/AVRO messages\n",
        "4. **Real-time Processing**: Process streaming data with windows\n",
        "5. **Kafka Sink**: Write results back to Kafka\n",
        "\n",
        "## üèóÔ∏è **Architecture Overview:**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Kafka Producer ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Kafka Topic    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Spark Streaming‚îÇ\n",
        "‚îÇ  (Python/Java)  ‚îÇ    ‚îÇ   (user-events)  ‚îÇ    ‚îÇ  (readStream)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                                          ‚îÇ\n",
        "                                                          ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Kafka Sink     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Processed Data  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Window/Aggregate‚îÇ\n",
        "‚îÇ  (results)      ‚îÇ    ‚îÇ  (with watermark)‚îÇ    ‚îÇ  (sliding/tumbling)‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## üìä **Use Cases:**\n",
        "- **Real-time Analytics**: Process events from Kafka in real-time\n",
        "- **Event Processing**: Transform and aggregate streaming events\n",
        "- **Data Pipeline**: Build end-to-end streaming pipelines\n",
        "- **Monitoring**: Real-time monitoring v√† alerting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow kafka-python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.admin import KafkaAdminClient, NewTopic\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session for Kafka Streaming\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkKafkaIntegration\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
        "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"üöÄ Spark Kafka Streaming Session initialized!\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"üìÅ Checkpoint Location: /tmp/kafka_checkpoint\")\n",
        "\n",
        "# Kafka Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
        "INPUT_TOPIC = \"spark-lab-events\"\n",
        "OUTPUT_TOPIC = \"spark-lab-results\"\n",
        "\n",
        "print(f\"\\nüì° Kafka Configuration:\")\n",
        "print(f\"   Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"   Input Topic: {INPUT_TOPIC}\")\n",
        "print(f\"   Output Topic: {OUTPUT_TOPIC}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Setup Kafka Topics\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Create Kafka topics for streaming\n",
        "- Verify topic creation\n",
        "- Understand topic configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Kafka Topics\n",
        "print(\"üìã Exercise 1: Setup Kafka Topics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # Create Kafka admin client\n",
        "    admin_client = KafkaAdminClient(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        client_id='spark-lab-admin'\n",
        "    )\n",
        "    \n",
        "    # Check if topics exist\n",
        "    existing_topics = admin_client.list_topics()\n",
        "    print(f\"\\nüìä Existing topics: {existing_topics}\")\n",
        "    \n",
        "    # Create topics if they don't exist\n",
        "    topics_to_create = []\n",
        "    \n",
        "    if INPUT_TOPIC not in existing_topics:\n",
        "        topics_to_create.append(\n",
        "            NewTopic(\n",
        "                name=INPUT_TOPIC,\n",
        "                num_partitions=3,\n",
        "                replication_factor=1\n",
        "            )\n",
        "        )\n",
        "        print(f\"‚úÖ Will create input topic: {INPUT_TOPIC}\")\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è  Input topic already exists: {INPUT_TOPIC}\")\n",
        "    \n",
        "    if OUTPUT_TOPIC not in existing_topics:\n",
        "        topics_to_create.append(\n",
        "            NewTopic(\n",
        "                name=OUTPUT_TOPIC,\n",
        "                num_partitions=3,\n",
        "                replication_factor=1\n",
        "            )\n",
        "        )\n",
        "        print(f\"‚úÖ Will create output topic: {OUTPUT_TOPIC}\")\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è  Output topic already exists: {OUTPUT_TOPIC}\")\n",
        "    \n",
        "    if topics_to_create:\n",
        "        admin_client.create_topics(new_topics=topics_to_create, validate_only=False)\n",
        "        print(f\"\\n‚úÖ Created {len(topics_to_create)} topic(s)\")\n",
        "        time.sleep(2)  # Wait for topics to be ready\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All topics already exist\")\n",
        "    \n",
        "    # List all topics\n",
        "    final_topics = admin_client.list_topics()\n",
        "    print(f\"\\nüìã All available topics: {final_topics}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Error setting up topics: {e}\")\n",
        "    print(\"   Note: Topics may need to be created manually or auto-creation is enabled\")\n",
        "    print(\"   You can create topics using:\")\n",
        "    print(f\"   kafka-topics --create --bootstrap-server localhost:9092 --topic {INPUT_TOPIC} --partitions 3 --replication-factor 1\")\n",
        "    print(f\"   kafka-topics --create --bootstrap-server localhost:9092 --topic {OUTPUT_TOPIC} --partitions 3 --replication-factor 1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Create Kafka Producer\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Create Kafka producer in Python\n",
        "- Send JSON messages to Kafka topic\n",
        "- Simulate real-time event stream\n",
        "- Understand producer configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Kafka Producer\n",
        "print(\"üì§ Exercise 2: Create Kafka Producer\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create producer\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "    key_serializer=lambda k: k.encode('utf-8') if k else None\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Kafka Producer created!\")\n",
        "\n",
        "# Function to generate sample events\n",
        "def generate_event(event_id, base_time=None):\n",
        "    \"\"\"Generate a sample event\"\"\"\n",
        "    if base_time is None:\n",
        "        base_time = datetime.now()\n",
        "    \n",
        "    event_time = base_time - timedelta(seconds=random.randint(0, 60))\n",
        "    \n",
        "    return {\n",
        "        'event_id': f'EVT_{event_id:04d}',\n",
        "        'user_id': f'user_{random.randint(1, 10)}',\n",
        "        'event_type': random.choice(['click', 'view', 'purchase', 'add_to_cart']),\n",
        "        'timestamp': event_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'amount': random.randint(10, 500) if random.random() > 0.6 else 0,\n",
        "        'product_id': f'product_{random.randint(1, 20)}',\n",
        "        'category': random.choice(['Electronics', 'Clothing', 'Books', 'Food'])\n",
        "    }\n",
        "\n",
        "# Send sample events\n",
        "print(\"\\nüì§ Sending sample events to Kafka...\")\n",
        "base_time = datetime.now()\n",
        "\n",
        "for i in range(20):\n",
        "    event = generate_event(i, base_time)\n",
        "    key = event['user_id']  # Use user_id as key for partitioning\n",
        "    \n",
        "    # Send to Kafka\n",
        "    future = producer.send(INPUT_TOPIC, key=key, value=event)\n",
        "    \n",
        "    # Optional: wait for confirmation\n",
        "    # record_metadata = future.get(timeout=10)\n",
        "    # print(f\"Sent: {event['event_id']} to partition {record_metadata.partition}\")\n",
        "    \n",
        "    time.sleep(0.1)  # Small delay to simulate real-time stream\n",
        "\n",
        "# Flush to ensure all messages are sent\n",
        "producer.flush()\n",
        "print(f\"‚úÖ Sent 20 events to topic: {INPUT_TOPIC}\")\n",
        "\n",
        "# Keep producer for later use\n",
        "print(\"\\nüí° Producer is ready. You can send more events later.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from Kafka v·ªõi Spark\n",
        "print(\"üì• Exercise 3: Read from Kafka v·ªõi Spark\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define schema for events\n",
        "event_schema = StructType([\n",
        "    StructField(\"event_id\", StringType()),\n",
        "    StructField(\"user_id\", StringType()),\n",
        "    StructField(\"event_type\", StringType()),\n",
        "    StructField(\"timestamp\", StringType()),\n",
        "    StructField(\"amount\", IntegerType()),\n",
        "    StructField(\"product_id\", StringType()),\n",
        "    StructField(\"category\", StringType())\n",
        "])\n",
        "\n",
        "# Read from Kafka\n",
        "print(f\"\\n1Ô∏è‚É£ Reading from Kafka topic: {INPUT_TOPIC}\")\n",
        "\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .option(\"failOnDataLoss\", \"false\") \\\n",
        "    .load()\n",
        "\n",
        "print(\"‚úÖ Kafka stream created!\")\n",
        "print(\"\\nüìã Kafka stream schema:\")\n",
        "kafka_stream.printSchema()\n",
        "\n",
        "# Parse JSON from Kafka value\n",
        "print(\"\\n2Ô∏è‚É£ Parsing JSON messages:\")\n",
        "\n",
        "parsed_stream = kafka_stream \\\n",
        "    .select(\n",
        "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
        "        col(\"value\").cast(\"string\").alias(\"json_value\"),\n",
        "        col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
        "        col(\"partition\"),\n",
        "        col(\"offset\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"kafka_key\"),\n",
        "        from_json(col(\"json_value\"), event_schema).alias(\"data\"),\n",
        "        col(\"kafka_timestamp\"),\n",
        "        col(\"partition\"),\n",
        "        col(\"offset\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"kafka_key\"),\n",
        "        col(\"data.*\"),\n",
        "        col(\"kafka_timestamp\"),\n",
        "        col(\"partition\"),\n",
        "        col(\"offset\")\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"event_timestamp\",\n",
        "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Parsed stream schema:\")\n",
        "parsed_stream.printSchema()\n",
        "\n",
        "# For demonstration, we'll use a memory sink to show data\n",
        "print(\"\\n3Ô∏è‚É£ Writing to memory sink to view data:\")\n",
        "print(\"   (In production, you'd use console, Kafka, or file sink)\")\n",
        "\n",
        "# Create a temporary view for querying\n",
        "query = parsed_stream \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"kafka_events\") \\\n",
        "    .start()\n",
        "\n",
        "# Wait a bit for data to arrive\n",
        "print(\"\\n‚è≥ Waiting for data to arrive...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Query the memory table\n",
        "if spark.catalog.tableExists(\"kafka_events\"):\n",
        "    print(\"\\nüìä Sample events from Kafka:\")\n",
        "    spark.sql(\"SELECT * FROM kafka_events LIMIT 10\").show(truncate=False)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No data in memory table yet. Try sending more events.\")\n",
        "\n",
        "# Stop the query for now (we'll restart it in next exercises)\n",
        "query.stop()\n",
        "print(\"\\n‚úÖ Query stopped. Ready for next exercises.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowed Aggregations v·ªõi Kafka Stream\n",
        "print(\"ü™ü Exercise 4: Windowed Aggregations v·ªõi Kafka Stream\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Recreate parsed stream with watermark\n",
        "print(\"\\n1Ô∏è‚É£ Creating stream with watermark:\")\n",
        "\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .load()\n",
        "\n",
        "parsed_with_watermark = kafka_stream \\\n",
        "    .select(\n",
        "        from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n",
        "    ) \\\n",
        "    .select(\"data.*\") \\\n",
        "    .withColumn(\n",
        "        \"event_timestamp\",\n",
        "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
        "    ) \\\n",
        "    .withWatermark(\"event_timestamp\", \"2 minutes\")  # 2-minute watermark\n",
        "\n",
        "print(\"‚úÖ Stream created with 2-minute watermark\")\n",
        "\n",
        "# Windowed aggregation\n",
        "print(\"\\n2Ô∏è‚É£ Tumbling window aggregation (2-minute windows):\")\n",
        "\n",
        "windowed_agg = parsed_with_watermark \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_timestamp\"), \"2 minutes\"),\n",
        "        col(\"event_type\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
        "        sum(\"amount\").alias(\"total_revenue\"),\n",
        "        avg(\"amount\").alias(\"avg_amount\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"event_type\"),\n",
        "        col(\"event_count\"),\n",
        "        col(\"unique_users\"),\n",
        "        col(\"total_revenue\"),\n",
        "        col(\"avg_amount\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Windowed aggregation defined\")\n",
        "\n",
        "# Write to console for demonstration\n",
        "print(\"\\n3Ô∏è‚É£ Starting streaming query (console output):\")\n",
        "print(\"   This will show results as windows complete\")\n",
        "\n",
        "query = windowed_agg \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", \"false\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Query started!\")\n",
        "print(\"\\nüí° Send more events to Kafka to see windowed aggregations\")\n",
        "print(\"   Query will run until stopped\")\n",
        "\n",
        "# Note: In notebook, query runs in background\n",
        "# You can check results in console or stop it\n",
        "time.sleep(10)  # Let it run for a bit\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Query is running. To stop it, run: query.stop()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sliding Windows v·ªõi Kafka\n",
        "print(\"üîÑ Exercise 5: Sliding Windows v·ªõi Kafka\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create stream with watermark\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .load()\n",
        "\n",
        "parsed_stream = kafka_stream \\\n",
        "    .select(\n",
        "        from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n",
        "    ) \\\n",
        "    .select(\"data.*\") \\\n",
        "    .withColumn(\n",
        "        \"event_timestamp\",\n",
        "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
        "    ) \\\n",
        "    .withWatermark(\"event_timestamp\", \"5 minutes\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Sliding window: 5-minute window, 1-minute slide\")\n",
        "\n",
        "sliding_window_agg = parsed_stream \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_timestamp\"), \"5 minutes\", \"1 minute\"),  # 5-min window, 1-min slide\n",
        "        col(\"category\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
        "        sum(\"amount\").alias(\"total_revenue\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"category\"),\n",
        "        col(\"event_count\"),\n",
        "        col(\"unique_users\"),\n",
        "        col(\"total_revenue\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Sliding window aggregation defined\")\n",
        "print(\"\\nüìä Window structure:\")\n",
        "print(\"   Window 1: [10:00 - 10:05)\")\n",
        "print(\"   Window 2: [10:01 - 10:06)  ‚Üê overlaps with Window 1\")\n",
        "print(\"   Window 3: [10:02 - 10:07)  ‚Üê overlaps with Window 1 & 2\")\n",
        "\n",
        "# Write to console\n",
        "sliding_query = sliding_window_agg \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", \"false\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "print(\"\\n‚úÖ Sliding window query started!\")\n",
        "print(\"üí° Send events to see overlapping windows in action\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Write Results to Kafka\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Write processed results back to Kafka\n",
        "- Configure Kafka sink\n",
        "- Understand output format\n",
        "- Build end-to-end Kafka pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write Results to Kafka\n",
        "print(\"üì§ Exercise 6: Write Results to Kafka\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create stream and process\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .load()\n",
        "\n",
        "parsed_stream = kafka_stream \\\n",
        "    .select(\n",
        "        from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n",
        "    ) \\\n",
        "    .select(\"data.*\") \\\n",
        "    .withColumn(\n",
        "        \"event_timestamp\",\n",
        "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
        "    ) \\\n",
        "    .withWatermark(\"event_timestamp\", \"2 minutes\")\n",
        "\n",
        "# Aggregate\n",
        "aggregated = parsed_stream \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_timestamp\"), \"2 minutes\"),\n",
        "        col(\"event_type\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        sum(\"amount\").alias(\"total_revenue\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").cast(\"string\").alias(\"window_start\"),\n",
        "        col(\"window.end\").cast(\"string\").alias(\"window_end\"),\n",
        "        col(\"event_type\"),\n",
        "        col(\"event_count\"),\n",
        "        col(\"total_revenue\")\n",
        "    )\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Writing aggregated results to Kafka topic:\")\n",
        "print(f\"   Output topic: {OUTPUT_TOPIC}\")\n",
        "\n",
        "# Convert to JSON string for Kafka\n",
        "output_stream = aggregated \\\n",
        "    .select(\n",
        "        to_json(struct([\n",
        "            col(\"window_start\"),\n",
        "            col(\"window_end\"),\n",
        "            col(\"event_type\"),\n",
        "            col(\"event_count\"),\n",
        "            col(\"total_revenue\")\n",
        "        ])).alias(\"value\")\n",
        "    )\n",
        "\n",
        "# Write to Kafka\n",
        "kafka_sink_query = output_stream \\\n",
        "    .writeStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"topic\", OUTPUT_TOPIC) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/kafka_sink_checkpoint\") \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Kafka sink query started!\")\n",
        "print(f\"   Results are being written to: {OUTPUT_TOPIC}\")\n",
        "print(\"\\nüí° You can consume from output topic to verify results\")\n",
        "print(\"   Example consumer code is shown below\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Consume Results from Kafka\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Create Kafka consumer to read results\n",
        "- Verify processed data\n",
        "- Understand consumer groups\n",
        "- Monitor streaming pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consume Results from Kafka\n",
        "print(\"üì• Exercise 7: Consume Results from Kafka\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create Kafka consumer\n",
        "consumer = KafkaConsumer(\n",
        "    OUTPUT_TOPIC,\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
        "    consumer_timeout_ms=5000,  # Timeout after 5 seconds\n",
        "    auto_offset_reset='earliest',\n",
        "    group_id='spark-lab-consumer'\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Consumer created for topic: {OUTPUT_TOPIC}\")\n",
        "print(\"\\nüìä Consuming messages (timeout: 5 seconds):\")\n",
        "\n",
        "results = []\n",
        "try:\n",
        "    for message in consumer:\n",
        "        result = message.value\n",
        "        results.append(result)\n",
        "        print(f\"   Window: {result.get('window_start')} - {result.get('window_end')}\")\n",
        "        print(f\"   Event Type: {result.get('event_type')}\")\n",
        "        print(f\"   Count: {result.get('event_count')}, Revenue: {result.get('total_revenue')}\")\n",
        "        print()\n",
        "        \n",
        "        if len(results) >= 5:  # Limit to 5 messages for demo\n",
        "            break\n",
        "except Exception as e:\n",
        "    print(f\"   No more messages or error: {e}\")\n",
        "\n",
        "consumer.close()\n",
        "\n",
        "if results:\n",
        "    print(f\"\\n‚úÖ Consumed {len(results)} result(s) from Kafka\")\n",
        "    print(\"\\nüìã Summary of results:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"   {i}. {result.get('event_type')}: {result.get('event_count')} events, ${result.get('total_revenue')}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No results consumed. Make sure:\")\n",
        "    print(\"   1. Spark query is running and processing data\")\n",
        "    print(\"   2. Data has been sent to input topic\")\n",
        "    print(\"   3. Windows have completed (check watermark)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Continuous Producer (Background Thread)\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Create continuous event producer\n",
        "- Run producer in background thread\n",
        "- Simulate real-time event stream\n",
        "- Test streaming pipeline end-to-end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continuous Producer (Background Thread)\n",
        "print(\"üîÑ Exercise 8: Continuous Producer (Background Thread)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Global flag to control producer\n",
        "producer_running = False\n",
        "producer_thread = None\n",
        "\n",
        "def continuous_producer():\n",
        "    \"\"\"Continuously send events to Kafka\"\"\"\n",
        "    global producer_running\n",
        "    \n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "        key_serializer=lambda k: k.encode('utf-8') if k else None\n",
        "    )\n",
        "    \n",
        "    event_id = 0\n",
        "    base_time = datetime.now()\n",
        "    \n",
        "    while producer_running:\n",
        "        event_id += 1\n",
        "        event = generate_event(event_id, base_time)\n",
        "        key = event['user_id']\n",
        "        \n",
        "        producer.send(INPUT_TOPIC, key=key, value=event)\n",
        "        \n",
        "        if event_id % 10 == 0:\n",
        "            print(f\"   üì§ Sent {event_id} events...\")\n",
        "        \n",
        "        time.sleep(1)  # Send 1 event per second\n",
        "    \n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "    print(\"   ‚úÖ Producer stopped\")\n",
        "\n",
        "def start_producer():\n",
        "    \"\"\"Start continuous producer in background\"\"\"\n",
        "    global producer_running, producer_thread\n",
        "    \n",
        "    if producer_running:\n",
        "        print(\"‚ö†Ô∏è  Producer is already running\")\n",
        "        return\n",
        "    \n",
        "    producer_running = True\n",
        "    producer_thread = threading.Thread(target=continuous_producer, daemon=True)\n",
        "    producer_thread.start()\n",
        "    print(\"‚úÖ Continuous producer started in background\")\n",
        "    print(\"   Sending 1 event per second to Kafka\")\n",
        "    print(\"   Run stop_producer() to stop\")\n",
        "\n",
        "def stop_producer():\n",
        "    \"\"\"Stop continuous producer\"\"\"\n",
        "    global producer_running\n",
        "    \n",
        "    if not producer_running:\n",
        "        print(\"‚ö†Ô∏è  Producer is not running\")\n",
        "        return\n",
        "    \n",
        "    producer_running = False\n",
        "    if producer_thread:\n",
        "        producer_thread.join(timeout=2)\n",
        "    print(\"‚úÖ Producer stopped\")\n",
        "\n",
        "print(\"\\nüí° Functions available:\")\n",
        "print(\"   - start_producer(): Start sending events continuously\")\n",
        "print(\"   - stop_producer(): Stop the producer\")\n",
        "print(\"\\nüìù Example usage:\")\n",
        "print(\"   start_producer()  # Start sending events\")\n",
        "print(\"   # ... run your Spark streaming queries ...\")\n",
        "print(\"   stop_producer()   # Stop when done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. **Kafka Topic Setup**: Create and configure Kafka topics\n",
        "2. **Kafka Producer**: Send events to Kafka topics from Python\n",
        "3. **Spark Kafka Source**: Read streaming data from Kafka\n",
        "4. **Schema Parsing**: Parse JSON messages from Kafka\n",
        "5. **Windowed Aggregations**: Apply windows v√† watermarking to Kafka streams\n",
        "6. **Sliding Windows**: Use overlapping windows v·ªõi real Kafka data\n",
        "7. **Kafka Sink**: Write processed results back to Kafka\n",
        "8. **Kafka Consumer**: Consume v√† verify results\n",
        "9. **Continuous Producer**: Simulate real-time event streams\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "- **Kafka Integration**: Spark can read v√† write to Kafka seamlessly\n",
        "- **Real-time Processing**: Process events as they arrive from Kafka\n",
        "- **Watermarking**: Essential for handling late data in Kafka streams\n",
        "- **End-to-End Pipeline**: Complete pipeline from Kafka ‚Üí Spark ‚Üí Kafka\n",
        "- **Producer/Consumer**: Use Python Kafka library for testing v√† monitoring\n",
        "\n",
        "### üöÄ Best Practices:\n",
        "- Use appropriate Kafka partitions for parallelism\n",
        "- Set watermark based on expected Kafka message delays\n",
        "- Monitor Kafka consumer lag\n",
        "- Use checkpointing for fault tolerance\n",
        "- Test v·ªõi continuous producer for realistic scenarios\n",
        "- Monitor Spark streaming query metrics\n",
        "\n",
        "### üìö Next Steps:\n",
        "- Experiment with different window sizes v√† slides\n",
        "- Test watermark behavior v·ªõi intentionally late Kafka messages\n",
        "- Monitor Kafka consumer groups v√† offsets\n",
        "- Explore Kafka Schema Registry integration\n",
        "- Build more complex pipelines v·ªõi multiple Kafka topics\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
