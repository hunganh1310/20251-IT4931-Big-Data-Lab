{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Spark Streaming with Kafka\n",
        "\n",
        "## üéØ **Learning Objectives:**\n",
        "- Master Spark Structured Streaming\n",
        "- Learn real-time data processing patterns\n",
        "- Practice Kafka integration with Spark\n",
        "- Understand streaming analytics concepts\n",
        "- Implement real-time aggregations\n",
        "\n",
        "## üìö **Key Concepts:**\n",
        "1. **Structured Streaming**: Real-time data processing framework\n",
        "2. **Kafka Integration**: Reading from Kafka topics\n",
        "3. **Stream Processing**: Continuous data transformation\n",
        "4. **Watermarking**: Handling late-arriving data\n",
        "5. **Checkpointing**: Ensuring fault tolerance\n",
        "\n",
        "## üèóÔ∏è **Architecture Overview:**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Kafka Topic   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Spark Streaming ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Real-time     ‚îÇ\n",
        "‚îÇ   (Stock Data)  ‚îÇ    ‚îÇ     Engine       ‚îÇ    ‚îÇ   Analytics     ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ                        ‚îÇ                        ‚îÇ\n",
        "         ‚ñº                        ‚ñº                        ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ High-frequency  ‚îÇ    ‚îÇ Stream Processing‚îÇ    ‚îÇ Output Sinks    ‚îÇ\n",
        "‚îÇ Data Producer   ‚îÇ    ‚îÇ ‚Ä¢ Aggregations   ‚îÇ    ‚îÇ ‚Ä¢ Console       ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Window Functions‚îÇ    ‚îÇ ‚Ä¢ Memory        ‚îÇ\n",
        "‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Watermarking   ‚îÇ    ‚îÇ ‚Ä¢ File System   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## üìä **Streaming Use Cases:**\n",
        "- **Real-time Analytics**: Live dashboards and metrics\n",
        "- **Alert Systems**: Immediate notifications and triggers\n",
        "- **Data Pipeline**: Continuous ETL processes\n",
        "- **Monitoring**: Real-time system health checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install pyspark findspark pandas numpy pyarrow kafka-python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session for Streaming\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkStreamingLab\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
        "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"üöÄ Spark Streaming Session initialized successfully!\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"üìÅ Checkpoint Location: /tmp/checkpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Event-Time vs Processing-Time\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand the difference between event-time and processing-time\n",
        "- Learn why event-time is important for accurate analytics\n",
        "- Practice working with event timestamps\n",
        "- Understand late-arriving data challenges\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "- **Event-Time**: Time when the event actually occurred (in the data)\n",
        "- **Processing-Time**: Time when Spark processes the event\n",
        "- **Late Data**: Events that arrive after their event-time window has closed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Sample Streaming Data with Event-Time\n",
        "print(\"üìä Exercise 1: Event-Time vs Processing-Time\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create sample data with event timestamps\n",
        "# In real scenarios, this would come from Kafka\n",
        "sample_events = []\n",
        "\n",
        "# Simulate events with event-time (when event occurred)\n",
        "# and processing delays\n",
        "base_time = datetime.now()\n",
        "\n",
        "for i in range(20):\n",
        "    # Event-time: when the event actually happened\n",
        "    event_time = base_time - timedelta(minutes=random.randint(0, 30))\n",
        "    \n",
        "    # Processing-time: when Spark receives it (with delay)\n",
        "    processing_delay = random.randint(0, 5)  # 0-5 minutes delay\n",
        "    processing_time = event_time + timedelta(minutes=processing_delay)\n",
        "    \n",
        "    sample_events.append({\n",
        "        'event_id': f'EVT_{i+1:04d}',\n",
        "        'event_time': event_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'processing_time': processing_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'value': random.randint(10, 100),\n",
        "        'category': random.choice(['A', 'B', 'C'])\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "events_df = spark.createDataFrame(sample_events)\n",
        "\n",
        "print(\"\\nüìã Sample Events (showing event-time vs processing-time):\")\n",
        "events_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nüí° Key Insight:\")\n",
        "print(\"   - Event-time: When the event actually occurred\")\n",
        "print(\"   - Processing-time: When Spark processes it (may be delayed)\")\n",
        "print(\"   - Late data: Events arriving after their time window\")\n",
        "print(\"\\n‚ö†Ô∏è Using processing-time can lead to incorrect results!\")\n",
        "print(\"   ‚úÖ Always use event-time for accurate analytics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Tumbling Windows (Fixed Windows)\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand tumbling windows (non-overlapping)\n",
        "- Learn to aggregate data in fixed time intervals\n",
        "- Practice event-time based windowing\n",
        "- Understand window boundaries\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "- **Tumbling Window**: Fixed-size, non-overlapping windows\n",
        "- **Window Duration**: Size of each window (e.g., 5 minutes)\n",
        "- **Window Start/End**: Boundaries of each window\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tumbling Windows Example\n",
        "print(\"ü™ü Exercise 2: Tumbling Windows (Fixed Windows)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create sample streaming data with timestamps\n",
        "streaming_data = []\n",
        "base_time = datetime(2024, 1, 1, 10, 0, 0)  # Start at 10:00\n",
        "\n",
        "for i in range(50):\n",
        "    # Events spread over 10 minutes\n",
        "    event_time = base_time + timedelta(minutes=i*0.2)\n",
        "    streaming_data.append({\n",
        "        'timestamp': event_time,\n",
        "        'user_id': f'user_{random.randint(1, 5)}',\n",
        "        'action': random.choice(['click', 'view', 'purchase']),\n",
        "        'amount': random.randint(10, 200) if random.random() > 0.7 else 0\n",
        "    })\n",
        "\n",
        "# Create DataFrame and convert timestamp\n",
        "stream_df = spark.createDataFrame(streaming_data)\n",
        "stream_df = stream_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "\n",
        "# Register as temporary view for SQL\n",
        "stream_df.createOrReplaceTempView(\"events\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Tumbling Window - 2 minute windows:\")\n",
        "print(\"   Each window is 2 minutes, non-overlapping\")\n",
        "print(\"   Windows: [10:00-10:02), [10:02-10:04), [10:04-10:06), ...\")\n",
        "\n",
        "tumbling_result = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '2 minutes') as window,\n",
        "        COUNT(*) as event_count,\n",
        "        COUNT(DISTINCT user_id) as unique_users,\n",
        "        SUM(amount) as total_amount\n",
        "    FROM events\n",
        "    GROUP BY window(timestamp, '2 minutes')\n",
        "    ORDER BY window\n",
        "\"\"\")\n",
        "\n",
        "tumbling_result.show(truncate=False)\n",
        "\n",
        "print(\"\\nüìä Window Structure:\")\n",
        "print(\"   - Window start: Beginning of 2-minute interval\")\n",
        "print(\"   - Window end: End of 2-minute interval\")\n",
        "print(\"   - No overlap between windows\")\n",
        "print(\"   - Each event belongs to exactly one window\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Sliding Windows (Overlapping Windows)\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand sliding windows (overlapping)\n",
        "- Learn to create windows with slide intervals\n",
        "- Practice aggregating over overlapping time ranges\n",
        "- Understand when to use sliding vs tumbling windows\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "- **Sliding Window**: Overlapping windows with a slide interval\n",
        "- **Window Duration**: Total size of the window\n",
        "- **Slide Interval**: How much the window moves forward\n",
        "- **Overlap**: Events can belong to multiple windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sliding Windows Example\n",
        "print(\"üîÑ Exercise 3: Sliding Windows (Overlapping Windows)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Sliding Window - 5 minute window, 1 minute slide:\")\n",
        "print(\"   Window size: 5 minutes\")\n",
        "print(\"   Slide interval: 1 minute\")\n",
        "print(\"   Windows overlap by 4 minutes\")\n",
        "\n",
        "sliding_result = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '5 minutes', '1 minute') as window,\n",
        "        COUNT(*) as event_count,\n",
        "        COUNT(DISTINCT user_id) as unique_users,\n",
        "        SUM(amount) as total_amount\n",
        "    FROM events\n",
        "    GROUP BY window(timestamp, '5 minutes', '1 minute')\n",
        "    ORDER BY window\n",
        "\"\"\")\n",
        "\n",
        "sliding_result.show(truncate=False)\n",
        "\n",
        "print(\"\\nüìä Window Structure:\")\n",
        "print(\"   Window 1: [10:00 - 10:05)\")\n",
        "print(\"   Window 2: [10:01 - 10:06)  ‚Üê overlaps with Window 1\")\n",
        "print(\"   Window 3: [10:02 - 10:07)  ‚Üê overlaps with Window 1 & 2\")\n",
        "print(\"   ...\")\n",
        "print(\"\\nüí° Key Differences:\")\n",
        "print(\"   - Tumbling: No overlap, each event in 1 window\")\n",
        "print(\"   - Sliding: Overlap, each event can be in multiple windows\")\n",
        "print(\"   - Sliding provides smoother, more continuous results\")\n",
        "print(\"   - Useful for moving averages, trend analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Watermarking for Late Data\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand watermarking mechanism\n",
        "- Learn to handle late-arriving data\n",
        "- Practice setting appropriate watermark thresholds\n",
        "- Understand when late data is dropped vs processed\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "- **Watermark**: Threshold for how late data can arrive\n",
        "- **Late Data**: Events arriving after watermark threshold\n",
        "- **State Management**: Spark maintains state for incomplete windows\n",
        "- **State Cleanup**: Watermark allows Spark to clean up old state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Watermarking Example\n",
        "print(\"üíß Exercise 4: Watermarking for Late Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create data with some late-arriving events\n",
        "late_data = []\n",
        "base_time = datetime(2024, 1, 1, 10, 0, 0)\n",
        "\n",
        "# Normal events (on-time)\n",
        "for i in range(30):\n",
        "    event_time = base_time + timedelta(minutes=i*0.5)\n",
        "    late_data.append({\n",
        "        'timestamp': event_time,\n",
        "        'event_id': f'EVT_{i+1:04d}',\n",
        "        'value': random.randint(10, 100),\n",
        "        'arrival_status': 'on-time'\n",
        "    })\n",
        "\n",
        "# Late-arriving events (arriving after their window)\n",
        "# These events have old timestamps but arrive late\n",
        "for i in range(5):\n",
        "    # Event happened 10 minutes ago but arriving now\n",
        "    event_time = base_time - timedelta(minutes=10-i)\n",
        "    late_data.append({\n",
        "        'timestamp': event_time,\n",
        "        'event_id': f'LATE_{i+1:04d}',\n",
        "        'value': random.randint(10, 100),\n",
        "        'arrival_status': 'late'\n",
        "    })\n",
        "\n",
        "late_df = spark.createDataFrame(late_data)\n",
        "late_df = late_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "late_df.createOrReplaceTempView(\"late_events\")\n",
        "\n",
        "print(\"\\nüìä Sample Data (including late events):\")\n",
        "late_df.orderBy(\"timestamp\").show(20, truncate=False)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Aggregation WITHOUT watermark (processes all data, including late):\")\n",
        "print(\"   ‚ö†Ô∏è Without watermark, Spark keeps state forever (memory issue)\")\n",
        "\n",
        "no_watermark = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '2 minutes') as window,\n",
        "        COUNT(*) as event_count,\n",
        "        SUM(value) as total_value\n",
        "    FROM late_events\n",
        "    GROUP BY window(timestamp, '2 minutes')\n",
        "    ORDER BY window\n",
        "\"\"\")\n",
        "\n",
        "no_watermark.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Aggregation WITH watermark (drops very late data):\")\n",
        "print(\"   ‚úÖ With 5-minute watermark, late data within 5 min is processed\")\n",
        "print(\"   ‚ùå Data arriving >5 minutes late is dropped\")\n",
        "\n",
        "# Note: In real streaming, this would be:\n",
        "# stream_df.withWatermark(\"timestamp\", \"5 minutes\")...\n",
        "# For batch demo, we show the concept\n",
        "print(\"\"\"\n",
        "# Real streaming example:\n",
        "stream_df = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\\\n",
        "    .option(\"subscribe\", \"events\") \\\\\n",
        "    .load()\n",
        "\n",
        "# Parse and add watermark\n",
        "events_with_watermark = stream_df \\\\\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\") \\\\\n",
        "    .withWatermark(\"timestamp\", \"5 minutes\")  # 5-minute watermark\n",
        "\n",
        "# Aggregate with window\n",
        "windowed = events_with_watermark \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"2 minutes\"),\n",
        "        col(\"category\")\n",
        "    ) \\\\\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüí° Watermark Behavior:\")\n",
        "print(\"   - Events within watermark threshold: ‚úÖ Processed\")\n",
        "print(\"   - Events beyond watermark threshold: ‚ùå Dropped\")\n",
        "print(\"   - State cleanup: Old windows beyond watermark are cleaned up\")\n",
        "print(\"   - Trade-off: Lower watermark = less late data, but more state cleanup\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Advanced Window Operations\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Combine windows with other aggregations\n",
        "- Use multiple windows in the same query\n",
        "- Practice window functions within windows\n",
        "- Understand output modes with windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Window Operations\n",
        "print(\"‚öôÔ∏è Exercise 5: Advanced Window Operations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Multiple aggregations in the same window:\")\n",
        "advanced_result = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '3 minutes') as window,\n",
        "        COUNT(*) as total_events,\n",
        "        COUNT(DISTINCT user_id) as unique_users,\n",
        "        SUM(amount) as total_revenue,\n",
        "        AVG(amount) as avg_amount,\n",
        "        MAX(amount) as max_amount,\n",
        "        MIN(amount) as min_amount\n",
        "    FROM events\n",
        "    GROUP BY window(timestamp, '3 minutes')\n",
        "    ORDER BY window\n",
        "\"\"\")\n",
        "\n",
        "advanced_result.show(truncate=False)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Window with category grouping:\")\n",
        "category_window = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '2 minutes') as window,\n",
        "        action as category,\n",
        "        COUNT(*) as count,\n",
        "        SUM(amount) as total\n",
        "    FROM events\n",
        "    GROUP BY window(timestamp, '2 minutes'), action\n",
        "    ORDER BY window, category\n",
        "\"\"\")\n",
        "\n",
        "category_window.show(truncate=False)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Window with filtering:\")\n",
        "filtered_window = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        window(timestamp, '2 minutes') as window,\n",
        "        COUNT(*) as total_events,\n",
        "        SUM(amount) as revenue\n",
        "    FROM events\n",
        "    WHERE amount > 0  -- Only purchases\n",
        "    GROUP BY window(timestamp, '2 minutes')\n",
        "    ORDER BY window\n",
        "\"\"\")\n",
        "\n",
        "filtered_window.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Output Modes with Windows\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand different output modes\n",
        "- Learn when to use Append vs Update vs Complete mode\n",
        "- Practice output modes with windowed aggregations\n",
        "- Understand state management implications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output Modes with Windows\n",
        "print(\"üì§ Exercise 6: Output Modes with Windows\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Append Mode (requires watermark):\")\n",
        "print(\"\"\"\n",
        "# Append mode: Only outputs new rows when window closes\n",
        "# Requires watermark to know when window is complete\n",
        "\n",
        "query = events_with_watermark \\\\\n",
        "    .withWatermark(\"timestamp\", \"5 minutes\") \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"2 minutes\"),\n",
        "        col(\"category\")\n",
        "    ) \\\\\n",
        "    .agg(count(\"*\").alias(\"count\")) \\\\\n",
        "    .writeStream \\\\\n",
        "    .outputMode(\"append\") \\\\\n",
        "    .format(\"console\") \\\\\n",
        "    .start()\n",
        "\n",
        "# Behavior:\n",
        "# - Outputs only when window closes (after watermark passes)\n",
        "# - Each window appears once in output\n",
        "# - Lower memory usage\n",
        "# - Cannot use with aggregations without watermark\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Update Mode:\")\n",
        "print(\"\"\"\n",
        "# Update mode: Outputs updated rows as they change\n",
        "# Works with or without watermark\n",
        "\n",
        "query = stream_df \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"2 minutes\"),\n",
        "        col(\"category\")\n",
        "    ) \\\\\n",
        "    .agg(count(\"*\").alias(\"count\")) \\\\\n",
        "    .writeStream \\\\\n",
        "    .outputMode(\"update\") \\\\\n",
        "    .format(\"console\") \\\\\n",
        "    .start()\n",
        "\n",
        "# Behavior:\n",
        "# - Outputs whenever a window result changes\n",
        "# - Can output same window multiple times (as data arrives)\n",
        "# - Works without watermark\n",
        "# - Higher memory usage (keeps all state)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Complete Mode:\")\n",
        "print(\"\"\"\n",
        "# Complete mode: Outputs all rows every trigger\n",
        "# Requires unbounded state\n",
        "\n",
        "query = stream_df \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"2 minutes\"),\n",
        "        col(\"category\")\n",
        "    ) \\\\\n",
        "    .agg(count(\"*\").alias(\"count\")) \\\\\n",
        "    .writeStream \\\\\n",
        "    .outputMode(\"complete\") \\\\\n",
        "    .format(\"console\") \\\\\n",
        "    .start()\n",
        "\n",
        "# Behavior:\n",
        "# - Outputs all windows every trigger\n",
        "# - Requires unbounded state (all data kept in memory)\n",
        "# - Use only when necessary (e.g., all-time aggregations)\n",
        "# - High memory usage\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüí° Output Mode Selection:\")\n",
        "print(\"   - Append: Use with watermark for windowed aggregations\")\n",
        "print(\"   - Update: Use for incremental updates, can work without watermark\")\n",
        "print(\"   - Complete: Use for small datasets or when you need all results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Real-World Streaming Pipeline\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Build a complete streaming pipeline\n",
        "- Combine windows, watermarking, and aggregations\n",
        "- Practice with realistic scenarios\n",
        "- Understand best practices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-World Streaming Pipeline Example\n",
        "print(\"üåê Exercise 7: Real-World Streaming Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìã Complete Streaming Pipeline Pattern:\")\n",
        "print(\"\"\"\n",
        "# Step 1: Read from Kafka\n",
        "stream_df = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\\\n",
        "    .option(\"subscribe\", \"user-events\") \\\\\n",
        "    .option(\"startingOffsets\", \"latest\") \\\\\n",
        "    .load()\n",
        "\n",
        "# Step 2: Parse JSON and extract fields\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType()),\n",
        "    StructField(\"event_type\", StringType()),\n",
        "    StructField(\"timestamp\", TimestampType()),\n",
        "    StructField(\"amount\", IntegerType())\n",
        "])\n",
        "\n",
        "events_df = stream_df \\\\\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# Step 3: Add watermark for late data handling\n",
        "events_with_watermark = events_df \\\\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\")  # 10-minute watermark\n",
        "\n",
        "# Step 4: Windowed aggregation with sliding window\n",
        "windowed_agg = events_with_watermark \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\", \"1 minute\"),  # 5-min window, 1-min slide\n",
        "        col(\"event_type\")\n",
        "    ) \\\\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
        "        sum(\"amount\").alias(\"total_revenue\"),\n",
        "        avg(\"amount\").alias(\"avg_amount\")\n",
        "    )\n",
        "\n",
        "# Step 5: Write to output (console, Kafka, or file)\n",
        "query = windowed_agg \\\\\n",
        "    .writeStream \\\\\n",
        "    .outputMode(\"update\") \\\\\n",
        "    .format(\"console\") \\\\\n",
        "    .option(\"truncate\", \"false\") \\\\\n",
        "    .trigger(processingTime='10 seconds') \\\\\n",
        "    .start()\n",
        "\n",
        "# Step 6: Wait for termination\n",
        "query.awaitTermination()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n‚úÖ Key Components:\")\n",
        "print(\"   1. Kafka source: Real-time data ingestion\")\n",
        "print(\"   2. Schema parsing: Extract structured data\")\n",
        "print(\"   3. Watermark: Handle late-arriving data\")\n",
        "print(\"   4. Sliding window: Continuous aggregations\")\n",
        "print(\"   5. Multiple metrics: Count, sum, avg, distinct\")\n",
        "print(\"   6. Output mode: Update for incremental results\")\n",
        "print(\"   7. Trigger: Control processing frequency\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### ‚úÖ What we learned:\n",
        "1. **Event-Time vs Processing-Time**: Understanding the difference and why event-time matters\n",
        "2. **Tumbling Windows**: Fixed-size, non-overlapping windows for periodic aggregations\n",
        "3. **Sliding Windows**: Overlapping windows for continuous, smooth aggregations\n",
        "4. **Watermarking**: Mechanism to handle late-arriving data and manage state\n",
        "5. **Advanced Window Operations**: Multiple aggregations, filtering, grouping\n",
        "6. **Output Modes**: Append, Update, and Complete modes for different use cases\n",
        "7. **Real-World Pipeline**: Complete streaming pipeline with best practices\n",
        "\n",
        "### üéØ Key Takeaways:\n",
        "- **Always use event-time** for accurate analytics, not processing-time\n",
        "- **Tumbling windows** for periodic reports (e.g., hourly sales)\n",
        "- **Sliding windows** for continuous metrics (e.g., moving averages)\n",
        "- **Watermarking** is essential for state management and late data handling\n",
        "- **Output mode selection** depends on use case and memory constraints\n",
        "- **Window size and slide** should match your business requirements\n",
        "\n",
        "### üöÄ Best Practices:\n",
        "- Set watermark threshold based on expected data delays\n",
        "- Use Append mode with watermark for windowed aggregations\n",
        "- Choose window size based on analysis needs (not too small, not too large)\n",
        "- Monitor state size and adjust watermark accordingly\n",
        "- Use sliding windows for smoother, more continuous results\n",
        "- Test with late-arriving data to validate watermark behavior\n",
        "\n",
        "### üìö Next Steps:\n",
        "- Practice with real Kafka streams\n",
        "- Experiment with different window sizes and slides\n",
        "- Test watermark behavior with intentionally late data\n",
        "- Monitor streaming query performance and state size\n",
        "- Explore advanced features (stream-stream joins, deduplication)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
