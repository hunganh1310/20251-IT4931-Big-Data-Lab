{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Spark + Iceberg Integration\n",
        "\n",
        "## ðŸŽ¯ **Learning Objectives:**\n",
        "- Integrate Spark with Apache Iceberg\n",
        "- Learn to read and write Iceberg tables from Spark\n",
        "- Understand schema evolution with Spark\n",
        "- Practice time travel queries\n",
        "- Implement partitioning strategies\n",
        "\n",
        "## ðŸ“š **Key Concepts:**\n",
        "1. **Iceberg Catalog**: Metadata management for Iceberg tables\n",
        "2. **Spark Extensions**: Iceberg Spark extensions for integration\n",
        "3. **Schema Evolution**: Add/modify columns without rewriting data\n",
        "4. **Time Travel**: Query historical snapshots\n",
        "5. **Partitioning**: Optimize queries with partitioning\n",
        "\n",
        "## ðŸ—ï¸ **Architecture Overview:**\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Spark SQL     â”‚â”€â”€â”€â–¶â”‚  Iceberg Catalog â”‚â”€â”€â”€â–¶â”‚  Iceberg Tables â”‚\n",
        "â”‚   (Processing)  â”‚    â”‚  (Metadata)       â”‚    â”‚  (Data Files)   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                        â”‚                        â”‚\n",
        "         â–¼                        â–¼                        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Read/Write      â”‚    â”‚ Schema Evolution  â”‚    â”‚ Time Travel     â”‚\n",
        "â”‚ Operations      â”‚    â”‚ â€¢ Add columns     â”‚    â”‚ â€¢ Snapshots     â”‚\n",
        "â”‚                 â”‚    â”‚ â€¢ Modify types    â”‚    â”‚ â€¢ History       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## ðŸ“Š **Use Cases:**\n",
        "- **Data Lakehouse**: Store large datasets with ACID guarantees\n",
        "- **Schema Evolution**: Evolve schemas without downtime\n",
        "- **Time Travel**: Query data at different points in time\n",
        "- **Partitioning**: Optimize query performance\n",
        "- **Integration**: Combine Spark processing with Iceberg storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "# Note: Iceberg Spark runtime JAR needs to be available\n",
        "# For production, download iceberg-spark-runtime-3.5_2.12-1.4.0.jar\n",
        "# and place it in Spark's jars directory or use --jars option\n",
        "\n",
        "%pip install pyspark findspark pandas numpy pyarrow\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "print(\"âœ… Dependencies installed and imported successfully!\")\n",
        "print(\"\\nâš ï¸ Note: Iceberg Spark runtime JAR is required for full functionality\")\n",
        "print(\"   Download from: https://iceberg.apache.org/releases/\")\n",
        "print(\"   Or use Spark packages: --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session with Iceberg Support\n",
        "# Note: In production, you need to add Iceberg Spark runtime JAR\n",
        "# This example shows the configuration needed\n",
        "\n",
        "warehouse_path = \"/tmp/spark_warehouse/iceberg\"\n",
        "os.makedirs(warehouse_path, exist_ok=True)\n",
        "\n",
        "# Build Spark session\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"SparkIcebergIntegration\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "\n",
        "# Uncomment when Iceberg JAR is available:\n",
        "# builder = builder.config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
        "# builder = builder.config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
        "# builder = builder.config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
        "# builder = builder.config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
        "# builder = builder.config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
        "# builder = builder.config(\"spark.sql.catalog.local.warehouse\", warehouse_path)\n",
        "\n",
        "spark = builder.getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"ðŸš€ Spark Session initialized!\")\n",
        "print(f\"ðŸ“Š Spark Version: {spark.version}\")\n",
        "print(f\"ðŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ðŸ’¾ Warehouse Path: {warehouse_path}\")\n",
        "print(\"\\nâš ï¸ Note: Full Iceberg functionality requires Iceberg Spark runtime JAR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Create Sample Data and Setup\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Create sample datasets for Iceberg integration\n",
        "- Understand data preparation for Iceberg tables\n",
        "- Set up Spark DataFrames for Iceberg operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Sample Data for Iceberg Integration\n",
        "print(\"ðŸ“Š Creating sample datasets for Spark + Iceberg Integration Lab...\")\n",
        "\n",
        "# Sample Sales Data with timestamps for time travel\n",
        "sales_data = []\n",
        "products = ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Camera', 'Monitor', 'Keyboard', 'Mouse']\n",
        "customers = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry']\n",
        "categories = ['Electronics', 'Accessories', 'Computing']\n",
        "\n",
        "base_date = datetime.now() - timedelta(days=365)\n",
        "\n",
        "for i in range(1000):\n",
        "    sale_date = base_date + timedelta(days=random.randint(0, 365))\n",
        "    sales_data.append({\n",
        "        'sale_id': f'SALE_{i+1:04d}',\n",
        "        'customer_name': random.choice(customers),\n",
        "        'product_name': random.choice(products),\n",
        "        'category': random.choice(categories),\n",
        "        'quantity': random.randint(1, 5),\n",
        "        'unit_price': round(random.uniform(50, 2000), 2),\n",
        "        'sale_date': sale_date.strftime('%Y-%m-%d'),\n",
        "        'sale_timestamp': sale_date.isoformat(),\n",
        "        'region': random.choice(['North', 'South', 'East', 'West', 'Central'])\n",
        "    })\n",
        "\n",
        "# Create Spark DataFrame\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "\n",
        "# Add calculated columns\n",
        "sales_df = sales_df.withColumn(\n",
        "    \"total_amount\", \n",
        "    col(\"quantity\") * col(\"unit_price\")\n",
        ").withColumn(\n",
        "    \"sale_year\",\n",
        "    year(to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
        ").withColumn(\n",
        "    \"sale_month\",\n",
        "    month(to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
        ")\n",
        "\n",
        "print(f\"âœ… Sample data created:\")\n",
        "print(f\"   ðŸ“Š Sales records: {len(sales_data)}\")\n",
        "print(\"\\nðŸ“‹ DataFrame Schema:\")\n",
        "sales_df.printSchema()\n",
        "print(\"\\nðŸ“Š Sample Data:\")\n",
        "sales_df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Iceberg Table (Example Pattern)\n",
        "print(\"ðŸ“ Exercise 2: Write to Iceberg Table\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Create Iceberg table using Spark SQL (when Iceberg JAR is available):\")\n",
        "print(\"\"\"\n",
        "# Example SQL to create Iceberg table:\n",
        "spark.sql('''\n",
        "    CREATE TABLE local.db.sales_iceberg (\n",
        "        sale_id STRING,\n",
        "        customer_name STRING,\n",
        "        product_name STRING,\n",
        "        category STRING,\n",
        "        quantity INT,\n",
        "        unit_price DOUBLE,\n",
        "        sale_date DATE,\n",
        "        sale_timestamp TIMESTAMP,\n",
        "        region STRING,\n",
        "        total_amount DOUBLE,\n",
        "        sale_year INT,\n",
        "        sale_month INT\n",
        "    )\n",
        "    USING ICEBERG\n",
        "    PARTITIONED BY (sale_year, sale_month)\n",
        "    TBLPROPERTIES (\n",
        "        'write.format.default' = 'parquet',\n",
        "        'write.parquet.compression-codec' = 'snappy'\n",
        "    )\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Write DataFrame to Iceberg table (when Iceberg JAR is available):\")\n",
        "print(\"\"\"\n",
        "# Example: Write DataFrame to Iceberg\n",
        "sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"path\", \"local.db.sales_iceberg\") \\\n",
        "    .save()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Append data to existing Iceberg table:\")\n",
        "print(\"\"\"\n",
        "# Example: Append new data\n",
        "new_sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"path\", \"local.db.sales_iceberg\") \\\n",
        "    .save()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4ï¸âƒ£ Write with partitioning:\")\n",
        "print(\"\"\"\n",
        "# Example: Write with partitionBy\n",
        "sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"sale_year\", \"sale_month\") \\\n",
        "    .option(\"path\", \"local.db.sales_iceberg\") \\\n",
        "    .save()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâš ï¸ Note: These operations require Iceberg Spark runtime JAR\")\n",
        "print(\"   For now, we'll demonstrate the concepts with regular Parquet format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Write to Parquet (similar pattern, works without Iceberg JAR)\n",
        "print(\"\\n5ï¸âƒ£ Write to Parquet format (similar to Iceberg pattern):\")\n",
        "\n",
        "parquet_path = f\"{warehouse_path}/sales_parquet\"\n",
        "sales_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"sale_year\", \"sale_month\") \\\n",
        "    .parquet(parquet_path)\n",
        "\n",
        "print(f\"âœ… Data written to: {parquet_path}\")\n",
        "\n",
        "# Read back to verify\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "print(f\"\\nðŸ“Š Records read back: {parquet_df.count()}\")\n",
        "print(\"\\nðŸ“‹ Sample data:\")\n",
        "parquet_df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Read from Iceberg Table\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Read data from Iceberg tables\n",
        "- Understand query patterns\n",
        "- Learn to filter partitioned data\n",
        "- Practice reading specific snapshots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from Iceberg Table (Example Pattern)\n",
        "print(\"ðŸ“– Exercise 3: Read from Iceberg Table\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Read entire Iceberg table (when Iceberg JAR is available):\")\n",
        "print(\"\"\"\n",
        "# Example: Read from Iceberg table\n",
        "iceberg_df = spark.read \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .load(\"local.db.sales_iceberg\")\n",
        "\n",
        "iceberg_df.show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Read with filters (partition pruning):\")\n",
        "print(\"\"\"\n",
        "# Example: Filter by partition columns (efficient)\n",
        "iceberg_df = spark.read \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .load(\"local.db.sales_iceberg\") \\\n",
        "    .filter(\"sale_year = 2024 AND sale_month = 1\")\n",
        "\n",
        "iceberg_df.show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Read using Spark SQL:\")\n",
        "print(\"\"\"\n",
        "# Example: Query Iceberg table with SQL\n",
        "spark.sql('''\n",
        "    SELECT \n",
        "        customer_name,\n",
        "        product_name,\n",
        "        SUM(total_amount) as total_spent\n",
        "    FROM local.db.sales_iceberg\n",
        "    WHERE sale_year = 2024\n",
        "    GROUP BY customer_name, product_name\n",
        "    ORDER BY total_spent DESC\n",
        "''').show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâš ï¸ Note: These operations require Iceberg Spark runtime JAR\")\n",
        "print(\"   For now, we'll demonstrate with Parquet format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Read from Parquet (similar pattern)\n",
        "print(\"\\n4ï¸âƒ£ Read from Parquet with partition pruning:\")\n",
        "\n",
        "# Read with partition filter (efficient)\n",
        "filtered_df = spark.read.parquet(parquet_path) \\\n",
        "    .filter(\"sale_year = 2024 AND sale_month = 1\")\n",
        "\n",
        "print(f\"ðŸ“Š Filtered records: {filtered_df.count()}\")\n",
        "filtered_df.show(5, truncate=False)\n",
        "\n",
        "# Read and aggregate\n",
        "print(\"\\n5ï¸âƒ£ Read and aggregate (similar to Iceberg SQL):\")\n",
        "aggregated_df = spark.read.parquet(parquet_path) \\\n",
        "    .filter(\"sale_year = 2024\") \\\n",
        "    .groupBy(\"customer_name\", \"product_name\") \\\n",
        "    .agg(sum(\"total_amount\").alias(\"total_spent\")) \\\n",
        "    .orderBy(desc(\"total_spent\"))\n",
        "\n",
        "aggregated_df.show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Schema Evolution\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Understand Iceberg schema evolution capabilities\n",
        "- Add new columns to existing tables\n",
        "- Modify column types\n",
        "- Practice schema changes without data rewrite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema Evolution (Example Pattern)\n",
        "print(\"ðŸ”„ Exercise 4: Schema Evolution\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Add new column to Iceberg table (when Iceberg JAR is available):\")\n",
        "print(\"\"\"\n",
        "# Example: Add new column using ALTER TABLE\n",
        "spark.sql('''\n",
        "    ALTER TABLE local.db.sales_iceberg\n",
        "    ADD COLUMN discount_amount DOUBLE\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Modify column type:\")\n",
        "print(\"\"\"\n",
        "# Example: Change column type\n",
        "spark.sql('''\n",
        "    ALTER TABLE local.db.sales_iceberg\n",
        "    ALTER COLUMN unit_price TYPE DECIMAL(10, 2)\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Rename column:\")\n",
        "print(\"\"\"\n",
        "# Example: Rename column\n",
        "spark.sql('''\n",
        "    ALTER TABLE local.db.sales_iceberg\n",
        "    RENAME COLUMN sale_id TO transaction_id\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4ï¸âƒ£ Drop column:\")\n",
        "print(\"\"\"\n",
        "# Example: Drop column\n",
        "spark.sql('''\n",
        "    ALTER TABLE local.db.sales_iceberg\n",
        "    DROP COLUMN sale_timestamp\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n5ï¸âƒ£ Add data with new schema:\")\n",
        "print(\"\"\"\n",
        "# Example: Write DataFrame with new column\n",
        "new_sales_df = sales_df.withColumn(\"discount_amount\", lit(0.0))\n",
        "new_sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"path\", \"local.db.sales_iceberg\") \\\n",
        "    .save()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâœ… Key Benefits of Iceberg Schema Evolution:\")\n",
        "print(\"   - Add columns without rewriting existing data\")\n",
        "print(\"   - Backward compatible: old queries still work\")\n",
        "print(\"   - Forward compatible: new queries work with old data\")\n",
        "print(\"   - No downtime required\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Time Travel Queries\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Query historical snapshots of data\n",
        "- Understand snapshot management\n",
        "- Practice time travel queries\n",
        "- Learn to rollback to previous versions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Travel Queries (Example Pattern)\n",
        "print(\"â° Exercise 5: Time Travel Queries\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Query table at specific snapshot (when Iceberg JAR is available):\")\n",
        "print(\"\"\"\n",
        "# Example: Query at snapshot ID\n",
        "spark.sql('''\n",
        "    SELECT *\n",
        "    FROM local.db.sales_iceberg VERSION AS OF 12345\n",
        "    WHERE sale_year = 2024\n",
        "''').show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Query table at specific timestamp:\")\n",
        "print(\"\"\"\n",
        "# Example: Query at timestamp\n",
        "spark.sql('''\n",
        "    SELECT *\n",
        "    FROM local.db.sales_iceberg TIMESTAMP AS OF '2024-01-15 10:00:00'\n",
        "    WHERE sale_year = 2024\n",
        "''').show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ List all snapshots:\")\n",
        "print(\"\"\"\n",
        "# Example: View snapshot history\n",
        "spark.sql('''\n",
        "    SELECT *\n",
        "    FROM local.db.sales_iceberg.snapshots\n",
        "    ORDER BY committed_at DESC\n",
        "''').show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4ï¸âƒ£ Compare data between snapshots:\")\n",
        "print(\"\"\"\n",
        "# Example: Compare current vs previous snapshot\n",
        "spark.sql('''\n",
        "    SELECT \n",
        "        current.sale_id,\n",
        "        current.total_amount as current_amount,\n",
        "        previous.total_amount as previous_amount,\n",
        "        current.total_amount - previous.total_amount as difference\n",
        "    FROM local.db.sales_iceberg current\n",
        "    FULL OUTER JOIN local.db.sales_iceberg VERSION AS OF 12345 previous\n",
        "        ON current.sale_id = previous.sale_id\n",
        "    WHERE current.total_amount != previous.total_amount\n",
        "''').show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâœ… Key Benefits of Time Travel:\")\n",
        "print(\"   - Query historical data at any point in time\")\n",
        "print(\"   - Audit trail of all changes\")\n",
        "print(\"   - Rollback to previous versions if needed\")\n",
        "print(\"   - Compare data across different time points\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Partitioning Strategies\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Understand Iceberg partitioning\n",
        "- Choose appropriate partitioning strategies\n",
        "- Learn about hidden partitioning\n",
        "- Practice partition evolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partitioning Strategies (Example Pattern)\n",
        "print(\"ðŸ“¦ Exercise 6: Partitioning Strategies\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Create table with identity partitioning:\")\n",
        "print(\"\"\"\n",
        "# Example: Partition by year and month\n",
        "spark.sql('''\n",
        "    CREATE TABLE local.db.sales_iceberg (\n",
        "        sale_id STRING,\n",
        "        customer_name STRING,\n",
        "        product_name STRING,\n",
        "        sale_date DATE,\n",
        "        total_amount DOUBLE,\n",
        "        sale_year INT,\n",
        "        sale_month INT\n",
        "    )\n",
        "    USING ICEBERG\n",
        "    PARTITIONED BY (sale_year, sale_month)\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Create table with bucket partitioning:\")\n",
        "print(\"\"\"\n",
        "# Example: Bucket by customer_name\n",
        "spark.sql('''\n",
        "    CREATE TABLE local.db.sales_iceberg (\n",
        "        sale_id STRING,\n",
        "        customer_name STRING,\n",
        "        product_name STRING,\n",
        "        total_amount DOUBLE\n",
        "    )\n",
        "    USING ICEBERG\n",
        "    PARTITIONED BY (bucket(10, customer_name))\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Create table with transform partitioning:\")\n",
        "print(\"\"\"\n",
        "# Example: Partition by date truncation\n",
        "spark.sql('''\n",
        "    CREATE TABLE local.db.sales_iceberg (\n",
        "        sale_id STRING,\n",
        "        customer_name STRING,\n",
        "        sale_timestamp TIMESTAMP,\n",
        "        total_amount DOUBLE\n",
        "    )\n",
        "    USING ICEBERG\n",
        "    PARTITIONED BY (days(sale_timestamp))\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4ï¸âƒ£ Evolve partitioning (change partition spec):\")\n",
        "print(\"\"\"\n",
        "# Example: Change partitioning strategy\n",
        "spark.sql('''\n",
        "    ALTER TABLE local.db.sales_iceberg\n",
        "    SET PARTITION SPEC (\n",
        "        sale_year,\n",
        "        sale_month,\n",
        "        region\n",
        "    )\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâœ… Key Benefits of Iceberg Partitioning:\")\n",
        "print(\"   - Hidden partitioning: partition values computed automatically\")\n",
        "print(\"   - Partition evolution: change partitioning without rewriting data\")\n",
        "print(\"   - Multiple partition strategies: identity, bucket, transform\")\n",
        "print(\"   - Query optimization: automatic partition pruning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Integration Best Practices\n",
        "\n",
        "### ðŸŽ¯ **Learning Objectives:**\n",
        "- Understand best practices for Spark + Iceberg integration\n",
        "- Learn performance optimization techniques\n",
        "- Practice error handling\n",
        "- Understand monitoring and maintenance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integration Best Practices\n",
        "print(\"â­ Exercise 7: Integration Best Practices\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ Optimize write operations:\")\n",
        "print(\"\"\"\n",
        "# Example: Use appropriate write mode and options\n",
        "sales_df.write \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"write-target-file-size-bytes\", 536870912)  # 512MB\n",
        "    .option(\"write.parquet.compression-codec\", \"snappy\") \\\n",
        "    .option(\"write.parquet.row-group-size-bytes\", 134217728)  # 128MB\n",
        "    .partitionBy(\"sale_year\", \"sale_month\") \\\n",
        "    .save(\"local.db.sales_iceberg\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£ Optimize read operations:\")\n",
        "print(\"\"\"\n",
        "# Example: Use partition pruning and column projection\n",
        "spark.read \\\n",
        "    .format(\"iceberg\") \\\n",
        "    .load(\"local.db.sales_iceberg\") \\\n",
        "    .filter(\"sale_year = 2024 AND sale_month = 1\") \\\n",
        "    .select(\"sale_id\", \"customer_name\", \"total_amount\") \\\n",
        "    .show()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Maintain table metadata:\")\n",
        "print(\"\"\"\n",
        "# Example: Expire old snapshots\n",
        "spark.sql('''\n",
        "    CALL local.system.expire_snapshots(\n",
        "        table => 'local.db.sales_iceberg',\n",
        "        older_than => TIMESTAMP '2024-01-01 00:00:00',\n",
        "        retain_last => 10\n",
        "    )\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4ï¸âƒ£ Compact small files:\")\n",
        "print(\"\"\"\n",
        "# Example: Rewrite data files to optimize size\n",
        "spark.sql('''\n",
        "    CALL local.system.rewrite_data_files(\n",
        "        table => 'local.db.sales_iceberg',\n",
        "        strategy => 'binpack'\n",
        "    )\n",
        "''')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nâœ… Best Practices Summary:\")\n",
        "print(\"   - Use appropriate partitioning for query patterns\")\n",
        "print(\"   - Optimize file sizes (aim for 512MB-1GB per file)\")\n",
        "print(\"   - Use column projection to reduce I/O\")\n",
        "print(\"   - Regularly expire old snapshots\")\n",
        "print(\"   - Compact small files periodically\")\n",
        "print(\"   - Monitor table metadata size\")\n",
        "print(\"   - Use appropriate compression codec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### âœ… What we learned:\n",
        "1. **Spark + Iceberg Setup**: Configure Spark session with Iceberg extensions\n",
        "2. **Write Operations**: Write DataFrames to Iceberg tables with partitioning\n",
        "3. **Read Operations**: Read from Iceberg tables with filters and aggregations\n",
        "4. **Schema Evolution**: Add/modify columns without rewriting data\n",
        "5. **Time Travel**: Query historical snapshots of data\n",
        "6. **Partitioning**: Implement efficient partitioning strategies\n",
        "7. **Best Practices**: Optimize performance and maintain tables\n",
        "\n",
        "### ðŸŽ¯ Key Takeaways:\n",
        "- Iceberg provides ACID transactions for data lakehouse\n",
        "- Schema evolution allows changes without downtime\n",
        "- Time travel enables querying historical data\n",
        "- Partitioning strategies optimize query performance\n",
        "- Integration with Spark enables scalable data processing\n",
        "\n",
        "### ðŸš€ Next Steps:\n",
        "- Install Iceberg Spark runtime JAR for full functionality\n",
        "- Practice with larger datasets\n",
        "- Explore advanced features (merge, delete, update)\n",
        "- Integrate with other tools (dbt, Great Expectations)\n",
        "- Deploy to production with proper monitoring\n",
        "\n",
        "### ðŸ“š Resources:\n",
        "- [Apache Iceberg Documentation](https://iceberg.apache.org/)\n",
        "- [Iceberg Spark Integration](https://iceberg.apache.org/docs/latest/spark-configuration/)\n",
        "- [Iceberg Spark Runtime](https://iceberg.apache.org/releases/)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
